{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f53fea",
   "metadata": {},
   "source": [
    "<center><strong><font size=+3>Introduction and examples of robust directional and multivariate median estimators</font></center>\n",
    "<br><br>\n",
    "</center>\n",
    "<center><strong><font size=+2>Matyas Molnar and Bojan Nikolic</font><br></strong></center>\n",
    "<br><center><strong><font size=+1>Astrophysics Group, Cavendish Laboratory, University of Cambridge</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset, zoomed_inset_axes\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats.mstats import gmean as geometric_mean\n",
    "\n",
    "from robstat.robstat import circ_mean_dev, geometric_median, mardia_median, \\\n",
    "mv_median, mv_normality, tukey_median\n",
    "from robstat.utils import round_down, round_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc94b1",
   "metadata": {},
   "source": [
    "## Directional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-shelter",
   "metadata": {},
   "source": [
    "### Mardia Median\n",
    "\n",
    "Mardia median given by the angle that the circular mean deviation:\n",
    "\n",
    "$$ d(\\tilde\\theta) = \\pi - \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\pi - \\left| \\theta_i - \\tilde\\theta \\right| \\right| $$\n",
    "\n",
    "where $\\tilde\\theta$ is the estimate of the preferred direction, and it is used as a measure of dispersion.\n",
    "\n",
    "The Mardia median occasionally leads to a non-unique estimate of the circular median since there can sometimes be two or more diameters that divide the data equally and have the same circular mean deviation.\n",
    "\n",
    "Weighted Mardia median:\n",
    "\n",
    "$$ d(\\tilde\\theta) = \\pi - \\frac{1}{\\sum \\eta_i} \\sum_{i=1}^{n} \\eta_i \\left| \\pi - \\left| \\theta_i - \\tilde\\theta \\right| \\right| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample angular data\n",
    "angle_center = 0.5\n",
    "np.random.seed(0)\n",
    "angles = angle_center + np.random.normal(loc=0, scale=0.2, size=200)\n",
    "\n",
    "x_coords = np.cos(angles)\n",
    "y_coords = np.sin(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6), dpi=100)\n",
    "\n",
    "circ_rad = 1\n",
    "lim_rng = circ_rad * 1.25\n",
    "\n",
    "ax.set(xlim=(-lim_rng, lim_rng), ylim = (-lim_rng, lim_rng))\n",
    "\n",
    "a_circle = plt.Circle((0, 0), 1, fill=False, color='blue', alpha=0.5)\n",
    "ax.add_artist(a_circle)\n",
    "\n",
    "ax.axhline(0, color='grey', ls='--', lw=0.5)\n",
    "ax.axvline(0, color='grey', ls='--', lw=0.5)\n",
    "ax.plot(x_coords, y_coords, 'o', color='orange', markersize=4, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# circular mean deviation\n",
    "np.array([circ_mean_dev(angles, angle_center)]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking uniqueness of Mardia Median\n",
    "\n",
    "x = mardia_median(angles, init_guess=angle_center-0.3)\n",
    "y = mardia_median(angles, init_guess=angle_center+0.3)\n",
    "\n",
    "print('Mardia Medians found:')\n",
    "print(x)\n",
    "print(y, '\\n')\n",
    "\n",
    "print('Circular mean: \\n{}\\n'.format(stats.circmean(angles, nan_policy='omit')))\n",
    "\n",
    "cmd_x = circ_mean_dev(angles, x).item()\n",
    "cmd_y = circ_mean_dev(angles, y).item()\n",
    "print('Circular mean deviations for the Mardia Medians (check):')\n",
    "print(cmd_x)\n",
    "print(cmd_y, '\\n')\n",
    "\n",
    "if ~np.isclose(x, y):\n",
    "    print('The Mardia Median in this example is not unique')\n",
    "    take_mean = 'Mean'\n",
    "else:\n",
    "    take_mean = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6), dpi=100, subplot_kw={'projection': 'polar'})\n",
    "\n",
    "ax.set(rlim=(0, lim_rng))\n",
    "ax.set_rlabel_position(-90)\n",
    "\n",
    "# ax.plot(np.linspace(0, 2*np.pi, 1000), np.ones(1000), color='blue', linestyle='-', alpha=0.5)\n",
    "ax.plot(angles, np.ones_like(angles), 'o', color='orange', markersize=4, alpha=0.5, zorder=0)\n",
    "ax.scatter(np.mean([x, y]), 1, zorder=1, label='{} Mardia median'.format(take_mean))\n",
    "\n",
    "ax.set_xlim(round_down(angles.min(), 1), round_up(angles.max(), 1))\n",
    "\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847367f",
   "metadata": {},
   "source": [
    "## Multivariate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a633b",
   "metadata": {},
   "source": [
    "### Geometric Median\n",
    "\n",
    "The geometric median is defined as the value of the argument $y$ that minimizes the sum of Euclidian distances between $y$ and all points $x_i$:\n",
    "\n",
    "$$ \\underset{y \\in \\mathbb{R}^d}{\\mathrm{arg\\,min}} \\sum_{i=1}^n || x_i - y ||  $$\n",
    "\n",
    "Properties & asides:\n",
    " - The geometric median has a breakdown point of 0.5: up to half of the sample data may be arbitrarily corrupted, and the median of the samples will still provide a robust estimator for the location of the uncorrupted data\n",
    " - The geometric median is unique whenever the points are not collinear\n",
    " - Weiszfeld's algorithm for faster computation\n",
    " \n",
    "We can also weight the geometric median (c.f. the Weber problem):\n",
    "\n",
    "$$ \\underset{y \\in \\mathbb{R}^d}{\\mathrm{arg\\,min}} \\sum_{i=1}^n \\eta_i || x_i - y ||  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43952b2e",
   "metadata": {},
   "source": [
    "### Tukey Median\n",
    "\n",
    "Tukey (1975) proposed the halfspace depth as a tool to visually describe bivariate datasets.\n",
    "\n",
    "For a finite set of data points $X_n = \\{x_1, ..., x_n\\}$ in $\\mathbb{R}^d$, the Tukey depth, or halfspace depth of any point $y \\in \\mathbb{R}^d$ determines how central the point is inside the data cloud.\n",
    "\n",
    "The halfspace depth of $y$ is defined as the minimal number of data points in any closed halfspace determined by a hyperplane through $y$:\n",
    "\n",
    "$$ hdepth(y; X_n) = \\underset{|| u || = 1}{\\mathrm{min}} \\# \\{i; u^{\\tau} x_i \\geq u^{\\tau} y \\} $$\n",
    "\n",
    "The set of all points with depth > k is called the kth depth region Dk. The halfspace depth regions form a sequence of nested polyhedra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.random.random(500).reshape(-1, 2)*2\n",
    "points = np.concatenate((points, np.random.random(50).reshape(-1, 2)+5))\n",
    "points[:, 0] += 2\n",
    "points[:, 1] += 2\n",
    "points_c = points[:, 0] + points[:, 1]*1j\n",
    "sample_gmean = geometric_mean(points_c)\n",
    "sample_gmed = geometric_median(points_c, weights=None)\n",
    "sample_tmed = tukey_median(points_c)['barycenter']\n",
    "bad_med = lambda x : np.nanmedian(x.real) + np.nanmedian(x.imag)*1j\n",
    "sample_bmed = bad_med(points_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_ests = list(zip([sample_gmean, sample_gmed, sample_tmed, sample_bmed], \\\n",
    "               ['Geometric Mean', 'Geometric Median', 'Tukey Median', 'Separate Median'], \\\n",
    "               ['ro', 'co', 'yo', 'bo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6), dpi=100)\n",
    "\n",
    "ax.scatter(points[:, 0], points[:, 1], alpha=0.5)\n",
    "for i, med_est in enumerate(med_ests):\n",
    "    ax.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "ax.set_xlim(1.5, 9)\n",
    "ax.set_ylim(1.5, 9)\n",
    "\n",
    "# zoomed in sub region of the original image\n",
    "axins = zoomed_inset_axes(ax, zoom=6, loc=4)\n",
    "axins.scatter(points[:, 0], points[:, 1], alpha=0.5)\n",
    "for i, med_est in enumerate(med_ests):\n",
    "    axins.plot(med_est[0].real, med_est[0].imag, med_est[2])\n",
    "\n",
    "x1 = round_down(np.min([med_est[0].real for med_est in med_ests]), 1)\n",
    "x2 = round_up(np.max([med_est[0].real for med_est in med_ests]), 1)\n",
    "y1 = round_down(np.min([med_est[0].imag for med_est in med_ests]), 1)\n",
    "y2 = round_up(np.max([med_est[0].imag for med_est in med_ests]), 1)\n",
    "axins.set_xlim(x1, x2)\n",
    "axins.set_ylim(y1, y2)\n",
    "\n",
    "axins.tick_params(axis='x', direction='in', pad=-15)\n",
    "mark_inset(ax, axins, loc1=1, loc2=3, fc='none', ec='0.5')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect of outliers in data\n",
    "\n",
    "print('Geometric median')\n",
    "print(geometric_median(points[:250], weights=None))\n",
    "print(geometric_median(points, weights=None), '\\n')\n",
    "\n",
    "print('Tukey median')\n",
    "print(tukey_median(points[:250], weights=None)['barycenter'])\n",
    "print(tukey_median(points, weights=None)['barycenter'], '\\n')\n",
    "\n",
    "print('Bad median')\n",
    "print(bad_med(points[:250]))\n",
    "print(bad_med(points))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57904d6",
   "metadata": {},
   "source": [
    "### Other location depth notions\n",
    "\n",
    " - Simplicial depth\n",
    " - Oja depth\n",
    " - Projection depth\n",
    " - Spatial depth\n",
    " \n",
    " See e.g. https://www.csun.edu/~ctoth/Handbook/chap58.pdf, https://cran.r-project.org/web/packages/depth/depth.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mvm in ['Tukey', 'Oja', 'Liu', 'Spatial', 'CWmed']:\n",
    "    print('{:7s}: {}'.format(mvm, mv_median(points, method=mvm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6468f",
   "metadata": {},
   "source": [
    "## Normality tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8130c9",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66ec73",
   "metadata": {},
   "source": [
    "The Shapiro–Wilk test (1965) tests the null hypothesis that a sample $x_1, \\dots, x_n$ comes from a normal distribution. The test statistic is given by\n",
    "\n",
    "$$ W = \\frac{\\left( \\sum_{i=1}^n a_i x_{\\left( i \\right)} \\right)^2}{\\sum_{i=1}^n \\left(x_i - \\bar{x} \\right)^2} $$\n",
    "\n",
    "where $x_{\\left( i \\right)}$ are the ordered sample values (i.e. the the $i$th-smallest number in the sample), $\\bar{x}$ is the sample mean, and $a_i$ are constants generated from the means, with $\\vec{a} = \\left( a_1, \\dots, a_n \\right)$ given by\n",
    "\n",
    "$$ \\vec{a} = \\frac{m^{\\intercal}V^{-1}}{C} $$\n",
    "\n",
    "with vector norm $C$\n",
    "\n",
    "$$ C = \\lVert V^{-1} m \\rVert = \\left(m^{\\intercal} V^{-1} V^{-1} m \\right)^{1/2} $$\n",
    "\n",
    "and $\\vec{m} = \\left( m_1, \\dots, m_n \\right)^\\intercal$ made of the expected values of the order statistics of *iid* random variables sampled from the standard normal distribution; finally, $V$ is the covariance matrix of those normal order statistics\n",
    "\n",
    "Small values of $W$ are evidence of departure from normality.\n",
    "\n",
    "The null-hypothesis of this test is that the sample is normally distributed. Thus, if the $p$ value is less than the chosen threshold level (usually 5%), then the null hypothesis is rejected and there is evidence that the sample data are not normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f54f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_x = shapiro(points[:, 0])\n",
    "shapiro_y = shapiro(points[:, 1])\n",
    "print(shapiro_x)\n",
    "print(shapiro_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60b135",
   "metadata": {},
   "source": [
    "### Henze-Zirkler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a113cee",
   "metadata": {},
   "source": [
    "The Henze-Zirkler test (1990) statistic is based on a non-negative functional that measures the distance between two distribution functions, the hypothesized function (which is the multivariate normal) and the observed function.\n",
    "\n",
    "The HZ test statistic is given by\n",
    "\n",
    "$$ HZ = \\frac{1}{n} \\sum^n_{i=1} \\sum^n_{j=1} e^{-\\frac{\\beta^2}{2} D_{ij}} - 2 (1 + \\beta^2)^{-\\frac{p}{2}} \\sum^n_{j=1} e^{-\\frac{\\beta^2}{2 (1+\\beta^2)} D_{i}} + n (1 + 2 \\beta^2)^{-\\frac{p}{2}} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ p = \\# \\; \\mathrm{variables} $$\n",
    "\n",
    "$$ \\beta = \\frac{1}{\\sqrt{2}} \\left( \\frac{n (2p + 1)}{4} \\right)^{\\frac{1}{p+4}} $$\n",
    "\n",
    "$$ D_{ij} = (x_i - x_j)' S^{-1} (x_i - x_j) $$\n",
    "\n",
    "$$ D_{i} = (x_i - \\bar{x})' S^{-1} (x_i - \\bar{x}) $$\n",
    "\n",
    "with $D_{i}$ giving the squared Mahalanobis distance of $i^{\\mathrm{th}}$ observation to the centroid and $D_{ij}$ the Mahalanobis distance between the $i^{\\mathrm{th}}$ and $j^{\\mathrm{th}}$ observations, as $S$ is the covariance matrix.\n",
    "\n",
    "If the data is multivariate normal, $HZ$ is approximately log-normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff74253",
   "metadata": {},
   "outputs": [],
   "source": [
    "MVN_res = mv_normality(points)\n",
    "print(MVN_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
