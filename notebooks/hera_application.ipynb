{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e795ca4e",
   "metadata": {},
   "source": [
    "<center><strong><font size=+3>Applications of robust 2D median estimators to HERA data</font></center>\n",
    "<br><br>\n",
    "</center>\n",
    "<center><strong><font size=+2>Matyas Molnar and Bojan Nikolic</font><br></strong></center>\n",
    "<br><center><strong><font size=+1>Astrophysics Group, Cavendish Laboratory, University of Cambridge</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "import textwrap\n",
    "from IPython.display import Video\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset, zoomed_inset_axes\n",
    "from scipy import signal\n",
    "from scipy.stats import chi2, shapiro\n",
    "from scipy.stats.mstats import gmean as geometric_mean\n",
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "\n",
    "from hera_cal.io import HERAData\n",
    "from hera_cal.redcal import get_reds\n",
    "from hera_cal.utils import JD2LST\n",
    "\n",
    "from robstat.ml import nan_interp1d, nan_interp2d\n",
    "from robstat.plotting import grid_heatmaps, row_heatmaps\n",
    "from robstat.robstat import Cmardia_median, geometric_median, mardia_median, mv_median, \\\n",
    "mv_normality, mv_outlier, tukey_median\n",
    "from robstat.stdstat import mad_clip, rsc_mean\n",
    "from robstat.utils import DATAPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figs = False\n",
    "if plot_figs:\n",
    "    import matplotlib as mpl\n",
    "    mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d057f5",
   "metadata": {},
   "source": [
    "### Load HERA visibility data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = os.path.join(DATAPATH, 'zen.2458098.43869.HH.OCRSA.uvh5')\n",
    "\n",
    "hd = HERAData(sample_data)\n",
    "data, flags, _ = hd.read()\n",
    "\n",
    "reds = get_reds(hd.antpos, pols=hd.pols)\n",
    "flat_bls = [bl for grp in reds for bl in grp if bl in data.keys()]\n",
    "reds = [grp for grp in reds if set(grp).issubset(flat_bls)]\n",
    "bl_dict = {k: i for i, k in enumerate(flat_bls)}\n",
    "\n",
    "data = {k: np.ma.array(v, mask=flags[k], fill_value=np.nan) for k, v \\\n",
    "        in data.items()}\n",
    "mdata = np.ma.empty((hd.Nfreqs, hd.Ntimes, hd.Nbls), fill_value=np.nan, \\\n",
    "                     dtype=complex)\n",
    "for i, bl in enumerate(flat_bls):\n",
    "    mdata[..., i] = data[bl].transpose()\n",
    "    \n",
    "data = mdata.filled() # dimensions (freqs, times, bls)\n",
    "flags = mdata.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb601d",
   "metadata": {},
   "source": [
    "### Redundant averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b775ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_bls = reds[0]\n",
    "slct_bl_idxs = np.array([bl_dict[slct_bl] for slct_bl in slct_bls])\n",
    "slct_data = data[..., slct_bl_idxs]\n",
    "slct_flags = flags[..., slct_bl_idxs]\n",
    "assert slct_flags.sum() == np.isnan(slct_data).sum()\n",
    "print('Looking at baselines redundant to {}'.format(slct_bls[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cad2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one time integration / frequency slice with high variance\n",
    "idxs = np.unravel_index(np.nanargmax(np.nanstd(slct_data, axis=-1)), \\\n",
    "                        slct_data.shape[:2])\n",
    "print('Selecting freq / time slice {}'.format(idxs))\n",
    "slct_data_slice = slct_data[idxs[0], idxs[1], :]\n",
    "\n",
    "flt_nan = lambda x: x[~np.isnan(x)]\n",
    "sample_gmean = geometric_mean(flt_nan(slct_data_slice))\n",
    "sample_gmed = geometric_median(slct_data_slice, keep_res=True)\n",
    "sample_tmed = tukey_median(slct_data_slice)['barycenter']\n",
    "sample_mmed = Cmardia_median(slct_data_slice)\n",
    "marg_med = lambda x : np.nanmedian(x.real) + np.nanmedian(x.imag)*1j\n",
    "sample_bmed = marg_med(slct_data_slice)\n",
    "sample_hmean = rsc_mean(slct_data_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f25452",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_ests = list(zip([sample_gmean, sample_gmed, sample_tmed, sample_mmed, sample_bmed, sample_hmean], \n",
    "               ['Geometric Mean', 'Geometric Median', 'Tukey Median', 'Mardia Median', \\\n",
    "                'Marginal Median', 'HERA Mean']))\n",
    "for me in med_ests:\n",
    "    print('{:17s}: {:4f}'.format(me[1], me[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "ax.scatter(slct_data_slice.real, slct_data_slice.imag, alpha=0.5)\n",
    "ax.plot(sample_gmean.real, sample_gmean.imag, 'co', label='Geometric Mean')\n",
    "ax.plot(sample_gmed.real, sample_gmed.imag, 'ro', label='Geometric Median')\n",
    "ax.plot(sample_tmed.real, sample_tmed.imag, 'yo', label='Tukey Median')\n",
    "ax.plot(sample_mmed.real, sample_mmed.imag, 'ko', label='Mardia Median')\n",
    "ax.plot(sample_bmed.real, sample_bmed.imag, 'bo', label='Marginal Median')\n",
    "ax.plot(sample_hmean.real, sample_hmean.imag, 'go', label='HERA Mean')\n",
    "\n",
    "ax.annotate(slct_bls[0], xy=(0.05, 0.05), xycoords='axes fraction')\n",
    "ax.set_xlabel(r'$\\mathfrak{Re} \\; (V)$')\n",
    "ax.set_ylabel(r'$\\mathfrak{Im}(V)$')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_int = np.where(~np.isnan(data).all(axis=(0, 2)))[0][0] # first non-nan index\n",
    "# perhaps find index with fewest nans?\n",
    "gmean_res = np.empty((data.shape[0], len(reds)), dtype=complex)\n",
    "gmed_res, tmed_res, mmed_res, bmed_res, hmean_res = \\\n",
    "    [np.empty_like(gmean_res) for _ in range(5)]\n",
    "\n",
    "gmed_bf, mmed_bf = None, None\n",
    "for bl, bl_grp in enumerate(reds):\n",
    "    slct_bl_idxs = np.array([bl_dict[slct_bl] for slct_bl in bl_grp])\n",
    "    for f, frow in enumerate(data[:, time_int, slct_bl_idxs]):\n",
    "        if np.isnan(frow).all():\n",
    "            gmean_ij = gmed_ij = tmed_ij = mmed_ij = bmed_ij = hmean_ij = np.nan\n",
    "        else:\n",
    "            gmean_bf = geometric_mean(flt_nan(frow))\n",
    "            gmed_bf = geometric_median(frow, init_guess=gmed_bf, keep_res=True)\n",
    "            tmed_bf = tukey_median(frow)['barycenter']\n",
    "            mmed_bf = Cmardia_median(frow, init_guess=None)\n",
    "            bmed_bf = marg_med(frow)\n",
    "            hmean_bf = rsc_mean(frow)\n",
    "        gmean_res[f, bl] = gmean_bf\n",
    "        gmed_res[f, bl] = gmed_bf\n",
    "        tmed_res[f, bl] = tmed_bf\n",
    "        mmed_res[f, bl] = mmed_bf\n",
    "        bmed_res[f, bl] = bmed_bf\n",
    "        hmean_res[f, bl] = hmean_bf\n",
    "        \n",
    "med_est_res = list(zip([i[1] for i in med_ests], \\\n",
    "                  [gmean_res, gmed_res, tmed_res, mmed_res, bmed_res, hmean_res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(10, 20), dpi=100)\n",
    "spec = gridspec.GridSpec(nrows=2*len(med_ests), figure=fig, ncols=2)\n",
    "\n",
    "axes = []\n",
    "for i in range(len(med_ests)):\n",
    "    ax1 = fig.add_subplot(spec[i*2:2+i*2, 0])\n",
    "    ax2 = fig.add_subplot(spec[i*2, 1])\n",
    "    ax3 = fig.add_subplot(spec[i*2+1, 1])\n",
    "    axes.append([ax1, ax2, ax3])\n",
    "\n",
    "color = [None for i in med_est_res]\n",
    "for m, med_est in enumerate(med_est_res):\n",
    "    for i, bl_grp in enumerate(range(len(reds))):\n",
    "        axes[m][0].plot(hd.freqs, med_est[1][:, i].real, color=color[m], \\\n",
    "                   label='{}'.format(reds[i][0]) + r'$\\mathfrak{Re}$')\n",
    "        c = axes[m][0].get_lines()[-1].get_color()\n",
    "        color[m] = next(axes[m][0]._get_lines.prop_cycler)['color']\n",
    "        axes[m][0].plot(hd.freqs, med_est[1][:, i].imag, color=c, \\\n",
    "                   label='{}'.format(reds[i][0]) + r'$\\mathfrak{Im}$', ls='--')\n",
    "        axes[m][1].plot(hd.freqs, np.abs(med_est[1][:, i]), color=c, \\\n",
    "                   label='{}'.format(reds[i][0]))\n",
    "        axes[m][2].plot(hd.freqs, np.angle(med_est[1][:, i]), color=c, \\\n",
    "                   label='{}'.format(reds[i][0]), ls='--')\n",
    "        axes[m][0].text(x=0.05, y=0.5, s=med_est[0], transform=axes[m][0].transAxes, \\\n",
    "                        fontsize=10, style='normal', weight='light')\n",
    "\n",
    "for ax in axes:\n",
    "    ax[0].set_ylabel(r'$V$')\n",
    "    ax[1].set_ylabel(r'$|V|$')\n",
    "    ax[2].set_ylabel(r'$\\varphi$')\n",
    "    \n",
    "for ax in axes[-1]:\n",
    "    ax.set_xlabel(r'$\\nu$')\n",
    "    \n",
    "axes[0][0].set_title('Cartesian')\n",
    "axes[0][1].set_title('Polar')\n",
    "\n",
    "for ax in axes[0]:\n",
    "    ax.legend(framealpha=0.5, loc=1)\n",
    "\n",
    "plt.suptitle('Median estimates for 14-m EW baselines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820193a",
   "metadata": {},
   "source": [
    "### LST averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4693d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_xd_data = np.load(os.path.join(DATAPATH, 'xd_vis_rph.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a809ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd_data = sample_xd_data['data'] # dimensions (days, freqs, times, bls)\n",
    "xd_flags = sample_xd_data['flags']\n",
    "xd_data[xd_flags] = np.nan\n",
    "\n",
    "xd_redg = sample_xd_data['redg']\n",
    "xd_times = sample_xd_data['times']\n",
    "xd_pol = sample_xd_data['pol'].item()\n",
    "xd_lsts = JD2LST(xd_times)*12/np.pi # in hours\n",
    "\n",
    "freqs = sample_xd_data['freqs']\n",
    "chans = sample_xd_data['chans']\n",
    "if chans[-1]%100 == 99:\n",
    "    plt_chans = np.append(chans, chans[-1]+1)\n",
    "else:\n",
    "    plt_chans = chans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_grp = 0 # only look at 0th baseline group\n",
    "\n",
    "slct_bl_idxs = np.where(xd_redg[:, 0] == bl_grp)[0]\n",
    "data = xd_data[..., slct_bl_idxs]\n",
    "flags = xd_flags[..., slct_bl_idxs]\n",
    "slct_red_bl = xd_redg[slct_bl_idxs[0], :][1:]\n",
    "print('Looking at baselines redundant to ({}, {}, \\'{}\\')'.\\\n",
    "      format(*slct_red_bl, xd_pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90852c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_bls = 4 # just pick the first four baselines from the selected baseline group\n",
    "\n",
    "# use 2 time integrations for each median, as done in HERA LST-binning\n",
    "new_no_tints = int(np.ceil(data.shape[2]/2))\n",
    "xd_gmed_res_t = np.empty((xd_data.shape[1], new_no_tints, no_bls), dtype=complex)\n",
    "xd_tmed_res_t, xd_bmed_res_t, xd_hmean_res_t = \\\n",
    "    [np.empty_like(xd_gmed_res_t) for _ in range(3)]\n",
    "\n",
    "gmed_ij = None\n",
    "for bl in range(no_bls):\n",
    "    xd_data_b = data[..., bl]\n",
    "    for freq in range(xd_data_b.shape[1]):\n",
    "        for tint in range(new_no_tints):\n",
    "            xd_data_bft = xd_data_b[:, freq, 2*tint:2*tint+2].flatten()\n",
    "            if np.isnan(xd_data_bft).all():\n",
    "                gmed_ft = tmed_ft = bmed_ft = hmean_ft = np.nan\n",
    "            else:\n",
    "                gmed_ft = geometric_median(xd_data_bft, init_guess=gmed_ij, \\\n",
    "                                           keep_res=True)\n",
    "                tmed_ft = tukey_median(xd_data_bft)['barycenter']\n",
    "                bmed_ft = marg_med(xd_data_bft)\n",
    "                hmean_ft = rsc_mean(xd_data_bft)\n",
    "            xd_gmed_res_t[freq, tint, bl] = gmed_ft\n",
    "            xd_tmed_res_t[freq, tint, bl] = tmed_ft\n",
    "            xd_bmed_res_t[freq, tint, bl] = bmed_ft\n",
    "            xd_hmean_res_t[freq, tint, bl] = hmean_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa855e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = [xd_gmed_res_t, xd_tmed_res_t, xd_bmed_res_t, xd_hmean_res_t]\n",
    "flt_arrs = []\n",
    "for arr in arrs:\n",
    "    nan_bl = np.isnan(arr).all(axis=(0, 1))\n",
    "    if nan_bl.any():\n",
    "        arr = np.delete(arr, np.where(nan_bl)[0], axis=-1)\n",
    "    flt_arrs.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692208fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_arrs = [[arr[..., i] for i in range(flt_arrs[0].shape[-1])] for arr in flt_arrs]\n",
    "titles = ['Geometric Median', 'Tukey Median', 'Marginal Median', 'HERA Mean']\n",
    "\n",
    "ylabels = [str(ylab) + '\\n\\nFrequency channel' for ylab in reds[bl_grp][:no_bls]]\n",
    "\n",
    "grid_heatmaps(grid_arrs, apply_np_fn='abs', titles=titles, ybase=25, \\\n",
    "              xlabels='Time bin', ylabels=ylabels, clip_pctile=1, yticklabels=plt_chans, \\\n",
    "              figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4bd05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_heatmaps(grid_arrs, apply_np_fn='angle', titles=titles, ybase=25, \\\n",
    "              xlabels='Time bin', ylabels=ylabels, yticklabels=plt_chans, figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_heatmaps(grid_arrs, apply_np_fn='real', titles=titles, ybase=25, \\\n",
    "              xlabels='Time bin', ylabels=ylabels, clip_pctile=1, yticklabels=plt_chans, \\\n",
    "              figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_heatmaps(grid_arrs, apply_np_fn='imag', titles=titles, ybase=25, \\\n",
    "              xlabels='Time bin', ylabels=ylabels, clip_pctile=1, yticklabels=plt_chans, \\\n",
    "              figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc010a9",
   "metadata": {},
   "source": [
    "### LST + redundant averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at 2 consecutive time integrations / 1 frequency slice with high variance\n",
    "idxs = np.unravel_index(np.nanargmax(np.nanstd(data[..., :-1, :], axis=(0, -1))), \\\n",
    "                        data.shape[1:-1])\n",
    "print('Selecting freq / %time slice: ({}, {}-{})'.format(idxs[0], idxs[1], idxs[1]+1))\n",
    "\n",
    "# Have visibilities across days for the same baseline (2 time bins)\n",
    "# flatten the data array and perform statistics on the whole dataset\n",
    "data_slice = data[:, idxs[0], idxs[1]:idxs[1]+2, :].flatten()\n",
    "\n",
    "xd_sample_gmean = geometric_mean(flt_nan(data_slice))\n",
    "xd_sample_gmed = geometric_median(data_slice, keep_res=True)\n",
    "xd_sample_tmed = tukey_median(data_slice)['barycenter']\n",
    "xd_sample_mmed = Cmardia_median(data_slice)\n",
    "xd_sample_bmed = marg_med(data_slice)\n",
    "xd_sample_hmean = rsc_mean(data_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f05842",
   "metadata": {},
   "source": [
    "Alternatively, we could take the median of the visibility amplitude and the Mardia median of the phase. While this is an improvement on doing the median on cartesian coordinates separately, it still does not wholly consider the complex data. The geometric median or the Tukey median would be preferable methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_ests = list(zip([xd_sample_gmean, xd_sample_gmed, xd_sample_tmed, xd_sample_mmed, \\\n",
    "                     xd_sample_bmed, xd_sample_hmean], \\\n",
    "               ['Geometric Mean', 'Geometric Median', 'Tukey Median', 'Mardia Median', \\\n",
    "                'Marginal Median', 'HERA Mean'], \\\n",
    "               ['co', 'ro', 'yo', 'ko', 'bo', 'go']))\n",
    "for me in med_ests:\n",
    "    print('{:17s}: {:4f}'.format(me[1], me[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fad85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.scatter(flt_nan(data_slice).real, flt_nan(data_slice).imag, alpha=0.5)\n",
    "for i, med_est in enumerate(med_ests):\n",
    "    ax.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "\n",
    "# zoomed in sub region of the original image\n",
    "axins = zoomed_inset_axes(ax, zoom=6, loc=4)\n",
    "axins.scatter(flt_nan(data_slice).real, flt_nan(data_slice).imag, alpha=0.5)\n",
    "for i, med_est in enumerate(med_ests):\n",
    "    axins.plot(med_est[0].real, med_est[0].imag, med_est[2])\n",
    "\n",
    "x1 = np.floor(np.min([i[0].real for i in med_ests[:-2]]))\n",
    "x2 = np.ceil(np.max([i[0].real for i in med_ests[:-2]]))\n",
    "y1 = np.floor(np.min([i[0].imag for i in med_ests[:-2]]))\n",
    "y2 = np.ceil(np.max([i[0].imag for i in med_ests[:-2]]))\n",
    "axins.set_xlim(x1, x2)\n",
    "axins.set_ylim(y1, y2)\n",
    "\n",
    "axins.tick_params(axis='x', direction='in', pad=-15)\n",
    "mark_inset(ax, axins, loc1=1, loc2=3, fc='none', ec='0.5')\n",
    "\n",
    "ax.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "            xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'))\n",
    "ax.set_xlabel(r'$\\mathfrak{Re} \\; (V)$')\n",
    "ax.set_ylabel(r'$\\mathfrak{Im} \\; (V)$')\n",
    "ax.set_title(textwrap.fill('Bivariate location estimators for redundant '\\\n",
    "    'visibilities aggregated across JDs', 60))\n",
    "\n",
    "ax.legend(loc=1, prop={'size': 8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a3965",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=flt_nan(data_slice).real, y=flt_nan(data_slice).imag, \\\n",
    "                  kind='kde', height=8, cmap='Blues', fill=True, space=0)\n",
    "g.set_axis_labels(r'$\\mathfrak{Re} \\; (V)$', r'$\\mathfrak{Im} \\; (V)$', size=14)\n",
    "for i, med_est in enumerate(med_ests):\n",
    "    g.ax_joint.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "legend_properties = {'size': 10}\n",
    "g.ax_joint.legend(prop=legend_properties, loc='upper right')\n",
    "g.ax_joint.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "    xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "    size=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bec739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 2 time integrations for each median, as done in HERA LST-binning\n",
    "new_no_tints = int(np.ceil(xd_data.shape[2]/2))\n",
    "xd_gmed_res = np.empty((xd_data.shape[1], new_no_tints), dtype=complex)\n",
    "xd_tmed_res, xd_bmed_res, xd_hmean_res = [np.empty_like(xd_gmed_res) for _ in range(3)]\n",
    "\n",
    "gmed_ij = None\n",
    "slct_bl_idxs = np.array([bl_dict[slct_bl] for slct_bl in reds[bl_grp]])\n",
    "xd_data_b = xd_data[..., slct_bl_idxs]\n",
    "for freq in range(xd_data_b.shape[1]):\n",
    "    for tint in range(new_no_tints):\n",
    "        xd_data_bft = xd_data_b[:, freq, 2*tint:2*tint+2, :].flatten()\n",
    "        if np.isnan(xd_data_bft).all():\n",
    "            gmed_ft = tmed_ft = bmed_ft = hmean_ft = np.nan\n",
    "        else:\n",
    "            gmed_ft = geometric_median(xd_data_bft, init_guess=gmed_ij, keep_res=True)\n",
    "            tmed_ft = tukey_median(xd_data_bft)['barycenter']\n",
    "            bmed_ft = marg_med(xd_data_bft)\n",
    "            hmean_ft = rsc_mean(xd_data_bft)\n",
    "        xd_gmed_res[freq, tint] = gmed_ft\n",
    "        xd_tmed_res[freq, tint] = tmed_ft\n",
    "        xd_bmed_res[freq, tint] = bmed_ft\n",
    "        xd_hmean_res[freq, tint] = hmean_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ad096",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = [xd_gmed_res, xd_tmed_res, xd_bmed_res, xd_hmean_res]\n",
    "\n",
    "tr_arrs = lambda x, np_fn: [getattr(np, np_fn)(i) for i in x]\n",
    "garrs = [tr_arrs(arrs, 'abs'), tr_arrs(arrs, 'angle'), tr_arrs(arrs, 'real'), tr_arrs(arrs, 'imag')]\n",
    "garrs = [[arr[i] for arr in garrs] for i in range(len(garrs[0]))]\n",
    "\n",
    "titles = ['Geometric Median', 'Tukey Median', 'Marginal Median', 'HERA Mean']\n",
    "ylabels = ['Amp', 'Phase', r'$\\mathfrak{Re}$', r'$\\mathfrak{Im}$']\n",
    "ylabels = [ylab + '\\n\\nFrequency channel' for ylab in ylabels]\n",
    "\n",
    "grid_heatmaps(garrs, titles=titles, figsize=(12, 10), ybase=25, clip_pctile=1, \\\n",
    "              xlabels='Time bin', yticklabels=plt_chans, ylabels=ylabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3a149",
   "metadata": {},
   "source": [
    "#### Smoothness of median results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e7f704",
   "metadata": {},
   "source": [
    "Calculate standard deviation of the distances between successive points in either frequency or time to get an idea of the smoothness of the location results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee968cf",
   "metadata": {},
   "source": [
    "##### Standard deviation of absolute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75dcef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in time\n",
    "t_smoothness = []\n",
    "for arr in arrs:\n",
    "    t_stds = np.empty(arr.shape[0])\n",
    "    for f in range(arr.shape[0]):\n",
    "        dists = np.abs(np.ediff1d(arr[f, :]))\n",
    "        t_stds[f] = np.nanstd(dists)\n",
    "    t_smoothness.append(np.nanmean(t_stds))\n",
    "print('Smoothness in time: \\n{}\\n{}\\n'.format(titles, t_smoothness))\n",
    "\n",
    "# in frequency\n",
    "f_smoothness = []\n",
    "for arr in arrs:\n",
    "    f_stds = np.empty(arr.shape[1])\n",
    "    for t in range(arr.shape[1]):\n",
    "        dists = np.abs(np.ediff1d(arr[:, t]))\n",
    "        f_stds[t] = np.nanstd(dists)\n",
    "    f_smoothness.append(np.nanmean(f_stds))\n",
    "print('Smoothness in frequency: \\n{}\\n{}'.format(titles, f_smoothness))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a7536",
   "metadata": {},
   "source": [
    "##### Standard deviation of complex differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb699276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in time\n",
    "t_smoothness = []\n",
    "for arr in arrs:\n",
    "    t_stds = np.empty(arr.shape[0])\n",
    "    for f in range(arr.shape[0]):\n",
    "        dists = np.ediff1d(arr[f, :])\n",
    "        t_stds[f] = np.nanstd(dists)\n",
    "    t_smoothness.append(np.nanmean(t_stds))\n",
    "print('Smoothness in time: \\n{}\\n{}\\n'.format(titles, t_smoothness))\n",
    "\n",
    "# in frequency\n",
    "f_smoothness = []\n",
    "for arr in arrs:\n",
    "    f_stds = np.empty(arr.shape[1])\n",
    "    for t in range(arr.shape[1]):\n",
    "        dists = np.ediff1d(arr[:, t])\n",
    "        f_stds[t] = np.nanstd(dists)\n",
    "    f_smoothness.append(np.nanmean(f_stds))\n",
    "print('Smoothness in frequency: \\n{}\\n{}'.format(titles, f_smoothness))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a81e4",
   "metadata": {},
   "source": [
    "#### Biggest difference in geometric median and HERA mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_slice = np.mean(flags, axis=(0, 3))\n",
    "even_end = int(np.floor(ok_slice.shape[1]/2))*2\n",
    "ok_slice_ = 0.5 * (ok_slice[:, :even_end:2] + ok_slice[:, 1:even_end:2]) # since 2 tints are used\n",
    "odd_arr = even_end != flags.shape[2]\n",
    "if even_end != flags.shape[2]:\n",
    "    ok_slice_ = np.append(ok_slice_, np.expand_dims(ok_slice[:, -1], 1), axis=1)\n",
    "ok_slice = ok_slice_ < 0.5 # only if less than 50% of flags are flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa00ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_idx = np.unravel_index(np.nanargmax(np.abs(xd_gmed_res[ok_slice] - xd_hmean_res[ok_slice])), \\\n",
    "                          xd_gmed_res.shape)\n",
    "print('Frequency/time slice {} shows strong deviation between the geometric median and the '\\\n",
    "      'HERA mean.'.format(bd_idx))\n",
    "bd_data = xd_data_b[:, bd_idx[0], 2*bd_idx[1]:2*bd_idx[1]+2, :].flatten()\n",
    "\n",
    "bd_med_ests = list(zip([xd_gmed_res[bd_idx], xd_hmean_res[bd_idx]], \\\n",
    "                       ['Geometric Median', 'HERA Mean'], \\\n",
    "                       ['ro', 'go']))\n",
    "\n",
    "g = sns.jointplot(x=flt_nan(bd_data).real, y=flt_nan(bd_data).imag, \\\n",
    "                  kind='kde', height=8, cmap='Blues', fill=True, space=0)\n",
    "g.set_axis_labels(r'$\\mathfrak{Re} \\; (V)$', r'$\\mathfrak{Im} \\; (V)$', size=14)\n",
    "for i, med_est in enumerate(bd_med_ests):\n",
    "    g.ax_joint.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "legend_properties = {'size': 10}\n",
    "g.ax_joint.legend(prop=legend_properties, loc='upper right')\n",
    "g.ax_joint.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "    xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "    size=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dbeaeb",
   "metadata": {},
   "source": [
    "#### More density plots\n",
    "\n",
    "To help visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_figs:\n",
    "     # pick ok data slices to plot as examples of redundant visibility distributions\n",
    "    ok_idxs = np.where(ok_slice)\n",
    "    index = np.random.choice(ok_idxs[0].shape[0], 2, replace=False)  \n",
    "    slice1 = ok_idxs[0][index[0]], ok_idxs[1][index[0]]\n",
    "    slice2 = ok_idxs[0][index[1]], ok_idxs[1][index[1]]\n",
    "    \n",
    "    print('Random slices: {} & {}'.format(slice1, slice2))\n",
    "    \n",
    "    for rnd_slice in (slice1, slice2):\n",
    "\n",
    "        bd_data = xd_data_b[:, rnd_slice[0], 2*rnd_slice[1]:2*rnd_slice[1]+2, :].flatten()    \n",
    "\n",
    "        rnd_med_ests = list(zip([xd_gmed_res[rnd_slice], xd_hmean_res[rnd_slice]], \\\n",
    "                                ['Geometric Median', 'HERA Mean'], \\\n",
    "                                ['ro', 'go']))   \n",
    "\n",
    "        g = sns.jointplot(x=flt_nan(bd_data).real, y=flt_nan(bd_data).imag, \\\n",
    "                      kind='kde', height=8, cmap='Blues', fill=True, space=0)\n",
    "        g.set_axis_labels(r'$\\mathfrak{Re} \\; (V)$', r'$\\mathfrak{Im} \\; (V)$', size=14)\n",
    "        for i, med_est in enumerate(rnd_med_ests):\n",
    "            g.ax_joint.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "        legend_properties = {'size': 10}\n",
    "        g.ax_joint.legend(prop=legend_properties, loc='upper right')\n",
    "        g.ax_joint.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "            xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "            size=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb10803",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_btint = 0\n",
    "\n",
    "def get_data(freq):\n",
    "    return xd_data_b[:, freq, 2*slct_btint:2*slct_btint+2, :]\n",
    "\n",
    "initd = get_data(0)\n",
    "g = sns.JointGrid(x=flt_nan(initd).real, y=flt_nan(initd).imag, height=8)\n",
    "g.set_axis_labels(r'$\\mathfrak{Re} \\; (V)$', r'$\\mathfrak{Im} \\; (V)$', size=14, labelpad=-2)\n",
    "\n",
    "re_lim = (np.floor(np.nanmin(xd_data_b.real)), np.ceil(np.nanmax(xd_data_b.real)))\n",
    "im_lim = (np.floor(np.nanmin(xd_data_b.imag)), np.ceil(np.nanmax(xd_data_b.imag)))\n",
    "\n",
    "def prep_axes(g, xlim, ylim):\n",
    "    g.ax_joint.clear()\n",
    "    g.ax_joint.set_xlim(xlim)\n",
    "    g.ax_joint.set_ylim(ylim)\n",
    "    g.ax_marg_x.clear()\n",
    "    g.ax_marg_x.set_xlim(xlim)\n",
    "    g.ax_marg_y.clear()\n",
    "    g.ax_marg_y.set_ylim(ylim)\n",
    "    plt.setp(g.ax_marg_x.get_xticklabels(), visible=False)\n",
    "    plt.setp(g.ax_marg_y.get_yticklabels(), visible=False)\n",
    "    plt.setp(g.ax_marg_x.yaxis.get_majorticklines(), visible=False)\n",
    "    plt.setp(g.ax_marg_x.yaxis.get_minorticklines(), visible=False)\n",
    "    plt.setp(g.ax_marg_y.xaxis.get_majorticklines(), visible=False)\n",
    "    plt.setp(g.ax_marg_y.xaxis.get_minorticklines(), visible=False)\n",
    "    plt.setp(g.ax_marg_x.get_yticklabels(), visible=False)\n",
    "    plt.setp(g.ax_marg_y.get_xticklabels(), visible=False)\n",
    "\n",
    "def animate(freq):\n",
    "    data_f = get_data(freq)\n",
    "    g.x, g.y = flt_nan(data_f).real, flt_nan(data_f).imag\n",
    "    prep_axes(g, re_lim, im_lim)\n",
    "    g.plot_joint(sns.kdeplot, cmap='Blues', fill=True)\n",
    "    g.plot_marginals(sns.kdeplot, shade=True)\n",
    "    g.set_axis_labels(r'$\\mathfrak{Re} \\; (V)$', r'$\\mathfrak{Im} \\; (V)$', size=14)\n",
    "\n",
    "    med_ests_f = list(zip([xd_gmed_res[freq, slct_btint], xd_hmean_res[freq, slct_btint]], \\\n",
    "                          ['Geometric Median', 'HERA Mean'], \\\n",
    "                          ['ro', 'go']))\n",
    "    for i, med_est in enumerate(med_ests_f):\n",
    "        g.ax_joint.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "    g.ax_joint.legend(prop={'size': 10}, loc='upper right')\n",
    "    g.ax_joint.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "        xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "        size=14)\n",
    "\n",
    "ani = FuncAnimation(g.fig, animate, frames=xd_data_b.shape[1], interval=750, repeat=False)\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "ani.save(os.path.join(temp_dir, 'kdes.mp4'), writer='ffmpeg', dpi=200)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "Video(os.path.join(temp_dir, 'kdes.mp4'), width=600, embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c141f87",
   "metadata": {},
   "source": [
    "### Test of normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1594632",
   "metadata": {},
   "source": [
    "#### Shapiro-Wilk test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d5ce6",
   "metadata": {},
   "source": [
    "We test the aggregated visibility data (over days, redundant baselines and consecutive time integrations) for normality using the Shapiro-Wilk test, to see if the data is Gaussian distributed for the $\\mathfrak{Re}$ and $\\mathfrak{Im}$ components separately, thus justifying the use of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21152f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_w_re = np.empty_like(xd_gmed_res, dtype=float)\n",
    "shapiro_w_im, shapiro_p_re, shapiro_p_im = [np.empty_like(shapiro_w_re) for _ in range(3)]\n",
    "for freq in range(xd_data_b.shape[1]):\n",
    "    for tint in range(new_no_tints):\n",
    "        xd_data_bft = flt_nan(xd_data_b[:, freq, 2*tint:2*tint+2, :].flatten())\n",
    "        if np.isnan(xd_data_bft).all():\n",
    "            re_shapiro_stat = im_shapiro_stat = re_shapiro_pval = re_shapiro_pval = np.nan\n",
    "        else:\n",
    "            re_shapiro = shapiro(xd_data_bft.real)\n",
    "            im_shapiro = shapiro(xd_data_bft.imag)\n",
    "            re_shapiro_stat = re_shapiro.statistic\n",
    "            im_shapiro_stat = im_shapiro.statistic\n",
    "            re_shapiro_pval = re_shapiro.pvalue\n",
    "            re_shapiro_pval = im_shapiro.pvalue\n",
    "        \n",
    "        shapiro_w_re[freq, tint] = re_shapiro.statistic\n",
    "        shapiro_w_im[freq, tint] = im_shapiro.statistic\n",
    "        shapiro_p_re[freq, tint] = re_shapiro.pvalue\n",
    "        shapiro_p_im[freq, tint] = im_shapiro.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ef1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [[r'$W \\; \\mathrm{statistic} \\; - \\; \\mathfrak{Re}(V)$', \\\n",
    "          r'$p \\; \\mathrm{value} \\; - \\; \\mathfrak{Re}(V)$'], \\\n",
    "          [r'$W \\; \\mathrm{statistic} \\; - \\; \\mathfrak{Im}(V)$', \\\n",
    "          r'$p \\; \\mathrm{value} \\; - \\; \\mathfrak{Im}(V)$']]\n",
    "grid_heatmaps([[shapiro_w_re, shapiro_p_re], [shapiro_w_im, shapiro_p_im]], \\\n",
    "             titles=titles, figsize=(14, 7), ybase=25, share_cbar=True, clip_pctile=1, \\\n",
    "             xlabels='Time bin', yticklabels=plt_chans, ylabels='Frequency channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0591bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example histograms for aggregated visibility data\n",
    "\n",
    "# picking frequency/time slice with worst shapiro statistic for Re visibilities\n",
    "re_shap_min = np.unravel_index(np.nanargmin(shapiro_p_re), shapiro_w_re.shape)\n",
    "print('Slice {} has Shapiro-Wilk test p value {:.5f} for the Re component.\\n'\\\n",
    "      .format(re_shap_min, shapiro_p_re[re_shap_min]))\n",
    "\n",
    "print('If the p value < the chosen alpha level (usually taken to be 0.05), then the null hypothesis '\\\n",
    "      'is rejected and there is evidence that the data tested are not normally distributed')\n",
    "\n",
    "hist_data = flt_nan(xd_data_b[:, re_shap_min[0], re_shap_min[1], :])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(14, 7))\n",
    "\n",
    "sns.histplot(hist_data.real, ax=ax[0], binwidth=2.5, kde=True)\n",
    "sns.histplot(hist_data.imag, ax=ax[1], binwidth=2.5, kde=True)\n",
    "\n",
    "ax[0].set_xlabel(r'$\\mathfrak{Re}(V)$')\n",
    "ax[1].set_xlabel(r'$\\mathfrak{Im}(V)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb612d7",
   "metadata": {},
   "source": [
    "#### Henze-Zirkler multivariate normality test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e4e02",
   "metadata": {},
   "source": [
    "We use the HZ test as this considers the entirety of the data. Note that many alternatives tests also exist and that a single statistic does not definitely conclude if the multivariate data is normality distributed or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAD-clipping about Re and Im separately, like HERA\n",
    "nan_flags = np.isnan(xd_data_b)\n",
    "re_clip_f = mad_clip(xd_data_b.real, axis=(0, 3), flags=nan_flags, verbose=True)[1]\n",
    "im_clip_f = mad_clip(xd_data_b.imag, axis=(0, 3), flags=nan_flags, verbose=True)[1]\n",
    "\n",
    "xd_data_bc = xd_data_b.copy()\n",
    "xd_data_bc[re_clip_f + im_clip_f] *= np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "hz_r = np.empty_like(shapiro_w_re)\n",
    "hz_p = np.empty_like(hz_r)\n",
    "hz_n = np.empty_like(hz_r, dtype=bool)\n",
    "\n",
    "hz_r_c = np.empty_like(hz_r)\n",
    "hz_p_c = np.empty_like(hz_r)\n",
    "hz_n_c = np.empty_like(hz_n)\n",
    "\n",
    "bool_dict = {'NO': False, 'YES': True, np.nan: False}\n",
    "\n",
    "for freq in range(xd_data_b.shape[1]):\n",
    "    for tint in range(new_no_tints):\n",
    "        xd_data_bft = flt_nan(xd_data_b[:, freq, 2*tint:2*tint+2, :].flatten())\n",
    "        xd_data_bcft = flt_nan(xd_data_bc[:, freq, 2*tint:2*tint+2, :].flatten())\n",
    "        \n",
    "        hz_res = mv_normality(xd_data_bft, method='hz')\n",
    "        hz_r[freq, tint] = hz_res['HZ']\n",
    "        hz_p[freq, tint] = hz_res['p value']\n",
    "        hz_n[freq, tint] = bool_dict[hz_res['MVN']]\n",
    "        \n",
    "        hz_res_c = mv_normality(xd_data_bcft, method='hz')\n",
    "        hz_r_c[freq, tint] = hz_res_c['HZ']\n",
    "        hz_p_c[freq, tint] = hz_res_c['p value']\n",
    "        hz_n_c[freq, tint] = bool_dict[hz_res_c['MVN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fe331",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [r'$HZ \\; \\mathrm{statistic}$', r'$p \\; \\mathrm{value}$', 'Normality']\n",
    "row_heatmaps([hz_r, hz_p, hz_n], titles=titles, figsize=(14, 7), share_cbar=False, \\\n",
    "             cbar_loc=None, clip_pctile=1, xlabels='Time bin', ylabel='Frequency channel', \\\n",
    "             yticklabels=plt_chans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAD-clipped data\n",
    "titles = [r'$HZ \\; \\mathrm{statistic}$', r'$p \\; \\mathrm{value}$', 'Normality']\n",
    "row_heatmaps([hz_r_c, hz_p_c, hz_n_c], titles=titles, figsize=(14, 7), share_cbar=False, \\\n",
    "             cbar_loc=None, clip_pctile=1, xlabels='Time bin', ylabel='Frequency channel', \\\n",
    "             yticklabels=plt_chans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fa77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking frequency/time slice with worst HZ statistic\n",
    "hz_p_min = np.unravel_index(np.nanargmin(hz_p), hz_p.shape)\n",
    "print('Slice {} has HZ test p value {:.5f}.\\n'\\\n",
    "      .format(hz_p_min, hz_p[hz_p_min]))\n",
    "\n",
    "print('If the p value < the chosen alpha level (usually taken to be 0.05), then the null hypothesis '\\\n",
    "      'is rejected and there is evidence that the data tested are not normally distributed')\n",
    "\n",
    "hz_data = flt_nan(xd_data_b[:, hz_p_min[0], 2*hz_p_min[1]:2*hz_p_min[1]+2, :].flatten())\n",
    "\n",
    "bhz_med_ests = list(zip([xd_gmed_res[hz_p_min], xd_hmean_res[hz_p_min]], \\\n",
    "                        ['Geometric Median', 'HERA Mean'], \\\n",
    "                        ['ro', 'go']))\n",
    "\n",
    "g = sns.jointplot(x=hz_data.real, y=hz_data.imag, \\\n",
    "                  kind='kde', height=8, cmap='Blues', fill=True, space=0)\n",
    "for i, med_est in enumerate(bhz_med_ests):\n",
    "    g.ax_joint.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "g.set_axis_labels(r'$\\mathfrak{Re} \\; (V)$', r'$\\mathfrak{Im} \\; (V)$', size=14)\n",
    "legend_properties = {'size': 10}\n",
    "g.ax_joint.legend(prop=legend_properties, loc='upper right')\n",
    "g.ax_joint.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "    xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "    size=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca147b8",
   "metadata": {},
   "source": [
    "### Multivariate outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7984ca",
   "metadata": {},
   "source": [
    "We use the robust Mahalanobis distance to detect outliers in the complex HERA data, as opposed to performing MAD-clipping on the $\\mathfrak{Re}$ and$\\mathfrak{Im}$ components separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12c8e9",
   "metadata": {},
   "source": [
    "#### Slice with worst HZ statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d242367",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvo_res = mv_outlier(hz_data)\n",
    "mvo_res.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAD-clipping about Re and Im separately\n",
    "re_clip_f_mvo = mad_clip(hz_data.real, verbose=True)[1]\n",
    "im_clip_f_mvo = mad_clip(hz_data.imag, verbose=True)[1]\n",
    "\n",
    "mvo_res['MAD-clip'] = re_clip_f_mvo + im_clip_f_mvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(14, 6), sharey=True)\n",
    "sns.scatterplot(x=hz_data.real, y=hz_data.imag, hue=mvo_res['Mahalanobis Distance'], ax=axes[0])\n",
    "sns.scatterplot(x=hz_data.real, y=hz_data.imag, hue=mvo_res['Outlier'], ax=axes[1])\n",
    "sns.scatterplot(x=hz_data.real, y=hz_data.imag, hue=mvo_res['MAD-clip'], ax=axes[2])\n",
    "axes[0].set_ylabel(r'$\\mathfrak{Im} \\; (V)$')\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(r'$\\mathfrak{Re} \\; (V)$')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 97.5% quantile of the chi-square distribution is classically taken for outlier threshold\n",
    "# let's look at a stricter threshold:\n",
    "chi2_quantile = 0.99\n",
    "strct_outliers = np.where(mvo_res['Mahalanobis Distance'].values > chi2.ppf(chi2_quantile, 2))[0]\n",
    "print('Outliers when taking the chi-square quantile to be {}% are:'.format(chi2_quantile*100))\n",
    "print(*hz_data[strct_outliers].tolist(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711d735",
   "metadata": {},
   "source": [
    "#### Sifting through the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3562661",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dp = xd_data_b.shape[0]*xd_data_b.shape[-1]*2\n",
    "mah_outliers = np.empty((xd_data.shape[1], new_no_tints, no_dp), dtype=bool)\n",
    "\n",
    "uf_xd_data = sample_xd_data['data'][..., slct_bl_idxs]\n",
    "\n",
    "for freq in range(xd_data_b.shape[1]):\n",
    "    for tint in range(new_no_tints):\n",
    "        xd_data_bft = uf_xd_data[:, freq, 2*tint:2*tint+2, :].flatten()\n",
    "        if np.isnan(xd_data_bft).all():\n",
    "            out_ft = np.empty(no_dp)*np.nan\n",
    "        else:\n",
    "            out_ft = mv_outlier(xd_data_bft)['Outlier']\n",
    "        if odd_arr and tint == new_no_tints-1:\n",
    "            out_ft = np.append(out_ft, np.zeros_like(out_ft).astype(bool))\n",
    "        mah_outliers[freq, tint] = out_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers = mah_outliers.sum(axis=-1)/mah_outliers.shape[-1]*100\n",
    "row_heatmaps(no_outliers, clip_pctile=2, xlabels='Time bin', ylabel='Frequency channel', \\\n",
    "             titles=['Percentage of of outliers found with the robust Mahalanobis distance '\\\n",
    "             'technique'], yticklabels=plt_chans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed536325",
   "metadata": {},
   "outputs": [],
   "source": [
    "if odd_arr:\n",
    "    tint_dim = new_no_tints - 1\n",
    "else:\n",
    "    tint_dim = new_no_tints\n",
    "\n",
    "cal_flags_xdbl = flags.sum(axis=(0, -1))\n",
    "cal_flags = cal_flags_xdbl[:, :even_end].reshape((flags.shape[1], tint_dim, -1)).sum(axis=-1)\n",
    "\n",
    "mad_flags_xdbl = (re_clip_f + im_clip_f).sum(axis=(0, -1))\n",
    "mad_flags = mad_flags_xdbl[:, :even_end].reshape((flags.shape[1], tint_dim, -1)).sum(axis=-1)\n",
    "\n",
    "comb_flags_xdbl = (flags + re_clip_f + im_clip_f).sum(axis=(0, -1))\n",
    "comb_flags = comb_flags_xdbl[:, :even_end].reshape((flags.shape[1], tint_dim, -1)).sum(axis=-1)\n",
    "\n",
    "if odd_arr:\n",
    "    cal_flags = np.append(cal_flags, np.expand_dims(cal_flags_xdbl[:, -1], 1), axis=1)\n",
    "    mad_flags = np.append(mad_flags, np.expand_dims(mad_flags_xdbl[:, -1], 1), axis=1)    \n",
    "    comb_flags = np.append(comb_flags, np.expand_dims(comb_flags_xdbl[:, -1], 1), axis=1)\n",
    "    \n",
    "cal_f_pct = cal_flags / mah_outliers.shape[-1]*100\n",
    "mad_f_pct = mad_flags / mah_outliers.shape[-1]*100\n",
    "comb_f_pct = comb_flags / mah_outliers.shape[-1]*100\n",
    "\n",
    "if odd_arr:\n",
    "    cal_f_pct[:, -1] *= 2\n",
    "    mad_f_pct[:, -1] *= 2\n",
    "    comb_f_pct[:, -1] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff5069",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=['Percentage of of flagged data from calibration', \\\n",
    "        'Percentage of of flagged data from MAD-clipping', \\\n",
    "        'Percentage of of flagged data from calibration + MAD-clipping']\n",
    "titles = [textwrap.fill(t, 40) for t in titles]\n",
    "\n",
    "row_heatmaps([cal_f_pct, mad_f_pct, comb_f_pct], clip_pctile=2, figsize=(14, 6), \\\n",
    "             titles=titles, xlabels='Time bin', ylabel='Frequency channel', yticklabels=plt_chans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884cbdd",
   "metadata": {},
   "source": [
    "### Statistical properties of location estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6469136",
   "metadata": {},
   "source": [
    "#### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ad547",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 8), sharey='row', sharex='col')\n",
    "\n",
    "axes[0][0].plot(np.abs(xd_gmed_res), alpha=0.7)\n",
    "axes[0][1].plot(np.abs(xd_hmean_res), alpha=0.7)\n",
    "axes[1][0].plot(np.angle(xd_gmed_res), alpha=0.7)\n",
    "axes[1][1].plot(np.angle(xd_hmean_res), alpha=0.7)\n",
    "\n",
    "axes[0][0].set_ylabel(r'$|V|$')\n",
    "axes[1][0].set_ylabel(r'$\\varphi$')\n",
    "\n",
    "axes[1][0].set_xlabel('Frequency channel')\n",
    "axes[1][1].set_xlabel('Frequency channel')\n",
    "\n",
    "axes[0][0].set_title('Geometric Median')\n",
    "axes[0][1].set_title('HERA Mean')\n",
    "\n",
    "for axr in axes:\n",
    "    for axc in axr:\n",
    "        axc.set_xticks(np.arange(plt_chans.size)[::25])\n",
    "        axc.set_xticklabels(plt_chans[::25])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a81ba0",
   "metadata": {},
   "source": [
    "##### Fill in gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690611e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid interpolation to replace nan values\n",
    "\n",
    "gmed_interp2 = nan_interp2d(xd_gmed_res)\n",
    "hmean_interp2 = nan_interp2d(xd_hmean_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01636d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_chans = np.where(np.isnan(xd_gmed_res).any(axis=1))[0]\n",
    "flt_chans = chans.copy()\n",
    "if 0 in nan_chans:\n",
    "    flt_chans = flt_chans[1:]\n",
    "if chans.size - 1 in nan_chans:\n",
    "    flt_chans = flt_chans[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464675e2",
   "metadata": {},
   "source": [
    "### Nonparametric kernel regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96111891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can do similarly for phases; alternatively do a 2D KDE with e.g.\n",
    "# statsmodels.nonparametric.kernel_density.KDEMultivariate\n",
    "\n",
    "kde_abs_gmed = np.empty_like(gmed_interp2, dtype=float)\n",
    "kde_abs_hmean = np.empty_like(kde_abs_gmed)\n",
    "\n",
    "for btint in range(gmed_interp2.shape[1]):\n",
    "    kde_gmed = KernelReg(endog=np.abs(gmed_interp2[:, btint]), exog=flt_chans, \\\n",
    "                         reg_type='ll', var_type='c', bw=[3])\n",
    "    kde_abs_gmed[:, btint] = kde_gmed.fit(flt_chans)[0]\n",
    "    \n",
    "    kde_hmean = KernelReg(endog=np.abs(hmean_interp2[:, btint]), exog=flt_chans, \\\n",
    "                          reg_type='ll', var_type='c', bw=[3])\n",
    "    kde_abs_hmean[:, btint] = kde_hmean.fit(flt_chans)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), sharex='col', sharey='row')\n",
    "\n",
    "axes[0][0].plot(flt_chans, kde_abs_gmed, alpha=0.7)\n",
    "axes[0][1].plot(flt_chans, kde_abs_hmean, alpha=0.7)\n",
    "\n",
    "gmed_residual = np.abs(gmed_interp2) - kde_abs_gmed\n",
    "axes[1][0].plot(flt_chans, gmed_residual, alpha=0.3)\n",
    "axes[1][0].plot(flt_chans, gmed_residual.mean(axis=1))\n",
    "\n",
    "hmean_residual = np.abs(hmean_interp2) - kde_abs_hmean\n",
    "axes[1][1].plot(flt_chans, hmean_residual, alpha=0.3)\n",
    "axes[1][1].plot(flt_chans, hmean_residual.mean(axis=1))\n",
    "\n",
    "axes[1][0].set_xlabel('Frequency channel')\n",
    "axes[1][1].set_xlabel('Frequency channel')\n",
    "axes[0][0].set_ylabel(r'$|V|$')\n",
    "axes[1][0].set_ylabel('Residual')\n",
    "\n",
    "axes[0][0].set_title('Geometric Median KDE')\n",
    "axes[0][1].set_title('HERA Mean KDE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49bfbf6",
   "metadata": {},
   "source": [
    "#### Allan deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fcc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import allantools\n",
    "    \n",
    "    f_resolution = np.median(np.ediff1d(freqs))\n",
    "    t_resolution = np.median(hd.integration_time)\n",
    "    \n",
    "    rate_exp = np.log10(f_resolution)\n",
    "    tau_min = np.ceil(np.abs(rate_exp))*np.sign(rate_exp)\n",
    "\n",
    "    taus = np.logspace(tau_min, tau_min+np.ceil(np.log10(gmed_interp2.shape[0])), 1000)\n",
    "    \n",
    "    gmed_ads = np.empty((int(np.floor(gmed_interp2.shape[0]/2)), gmed_interp2.shape[1]))\n",
    "    hmean_ads = np.empty_like(gmed_ads)\n",
    "    \n",
    "    for btint in range(gmed_interp2.shape[1]):\n",
    "        # do OADEV on residuals rather than on signal with structure\n",
    "        gmed_taus2, gmed_ad, gmed_ade, gmed_ns = allantools.oadev(gmed_residual[:, btint], \\\n",
    "            rate=1/f_resolution, data_type='freq', taus=taus)\n",
    "\n",
    "        hmean_taus2, hmean_ad, hmean_ade, hmean_ns = allantools.oadev(hmean_residual[:, btint], \\\n",
    "            rate=1/f_resolution, data_type='freq', taus=taus)\n",
    "        \n",
    "        gmed_ads[:, btint] = gmed_ad\n",
    "        hmean_ads[:, btint] = hmean_ad\n",
    "        \n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(10, 6), sharey=True)\n",
    "    \n",
    "    ax[0].loglog(gmed_taus2, gmed_ads, alpha=0.5)\n",
    "    ax[1].loglog(hmean_taus2, hmean_ads, alpha=0.5)\n",
    "    \n",
    "    ax[0].set_title('Geometric Median')\n",
    "    ax[1].set_title('HERA Mean')\n",
    "    \n",
    "    ax[0].set_ylabel('Overlapping Allan deviation')\n",
    "    ax[0].set_xlabel(r'$\\tau$')\n",
    "    ax[1].set_xlabel(r'$\\tau$')\n",
    "    \n",
    "    plt.suptitle('Allan deviation')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    # get AllanTools package here https://github.com/aewallin/allantools\n",
    "    # or do pip install allantools\n",
    "    print('AllanTools package not installed - skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a2c8c",
   "metadata": {},
   "source": [
    "#### Power spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36980c6",
   "metadata": {},
   "source": [
    "##### Single time integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmed_delay, gmed_pspec = signal.periodogram(gmed_interp2[:, 0], fs=1/f_resolution, \\\n",
    "    window='hann', scaling='spectrum', nfft=None, detrend=False, \\\n",
    "    return_onesided=False)\n",
    "\n",
    "delay_sort = np.argsort(gmed_delay)\n",
    "gmed_delay = gmed_delay[delay_sort]\n",
    "gmed_pspec = gmed_pspec[delay_sort]\n",
    "\n",
    "hmean_delay, hmean_pspec = signal.periodogram(hmean_interp2[:, 0], fs=1./f_resolution, \\\n",
    "    window='hann', scaling='spectrum', nfft=None, detrend=False, \\\n",
    "    return_onesided=False)\n",
    "\n",
    "delay_sort = np.argsort(hmean_delay)\n",
    "hmean_delay = hmean_delay[delay_sort]\n",
    "hmean_pspec = hmean_pspec[delay_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef022c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(gmed_delay, gmed_pspec, label='Geometric Median', alpha=0.8)\n",
    "ax.plot(hmean_delay, hmean_pspec, label='HERA Mean', alpha=0.8)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('Power spectrum')\n",
    "ax.set_xlabel('Delay')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6687d76",
   "metadata": {},
   "source": [
    "##### All time integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a088eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmed_delay, gmed_pspec = signal.periodogram(gmed_interp2, fs=1/f_resolution, \\\n",
    "    window='hann', scaling='spectrum', nfft=None, detrend=False, \\\n",
    "    return_onesided=False, axis=0)\n",
    "\n",
    "delay_sort = np.argsort(gmed_delay)\n",
    "gmed_delay = gmed_delay[delay_sort]\n",
    "gmed_pspec = gmed_pspec[delay_sort, :]\n",
    "\n",
    "hmean_delay, hmean_pspec = signal.periodogram(hmean_interp2, fs=1/f_resolution, \\\n",
    "    window='hann', scaling='spectrum', nfft=None, detrend=False, \\\n",
    "    return_onesided=False, axis=0)\n",
    "\n",
    "delay_sort = np.argsort(hmean_delay)\n",
    "hmean_delay = hmean_delay[delay_sort]\n",
    "hmean_pspec = hmean_pspec[delay_sort, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a7ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(10, 6), sharey=True)\n",
    "\n",
    "axes[0].plot(gmed_delay, gmed_pspec, alpha=0.3)\n",
    "axes[0].plot(gmed_delay, gmed_pspec.mean(axis=1), alpha=1, color='orange')\n",
    "axes[0].set_ylabel('Power spectrum')\n",
    "\n",
    "axes[1].plot(hmean_delay, hmean_pspec, alpha=0.3)\n",
    "axes[1].plot(hmean_delay, hmean_pspec.mean(axis=1), alpha=1, color='purple')\n",
    "\n",
    "axes[2].plot(gmed_delay, gmed_pspec.mean(axis=1), alpha=0.8, color='orange', label='Geometric Median')\n",
    "axes[2].plot(hmean_delay, hmean_pspec.mean(axis=1), alpha=0.8, color='purple', label='HERA Mean')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Delay')\n",
    "    \n",
    "axes[0].set_title('Geometric Median')\n",
    "axes[1].set_title('HERA Mean')\n",
    "axes[2].set_title('Comparison')\n",
    "axes[2].legend(loc='best')\n",
    "\n",
    "plt.suptitle('Power spectra')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
