{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e795ca4e",
   "metadata": {},
   "source": [
    "<center><strong><font size=+3>Applications of robust 2D median estimators to HERA data</font></center>\n",
    "<br><br>\n",
    "</center>\n",
    "<center><strong><font size=+2>Matyas Molnar and Bojan Nikolic</font><br></strong></center>\n",
    "<br><center><strong><font size=+1>Astrophysics Group, Cavendish Laboratory, University of Cambridge</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset, zoomed_inset_axes\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats.mstats import gmean as geometric_mean\n",
    "\n",
    "from hera_cal.io import HERAData\n",
    "from hera_cal.redcal import get_reds\n",
    "\n",
    "from robstat.plotting import grid_heatmaps, row_heatmaps\n",
    "from robstat.robstat import Cmardia_median, geometric_median, mardia_median, mv_median, \\\n",
    "mv_normality, tukey_median\n",
    "from robstat.stdstat import mad_clip, rsc_mean\n",
    "from robstat.utils import DATAPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figs = False\n",
    "if plot_figs:\n",
    "    import matplotlib as mpl\n",
    "    mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d057f5",
   "metadata": {},
   "source": [
    "### Load HERA visibility data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = os.path.join(DATAPATH, 'zen.2458098.43869.HH.OCRSA.uvh5')\n",
    "\n",
    "hd = HERAData(sample_data)\n",
    "data, flags, _ = hd.read()\n",
    "\n",
    "reds = get_reds(hd.antpos, pols=hd.pols)\n",
    "flat_bls = [bl for grp in reds for bl in grp if bl in data.keys()]\n",
    "reds = [grp for grp in reds if set(grp).issubset(flat_bls)]\n",
    "bl_dict = {k: i for i, k in enumerate(flat_bls)}\n",
    "\n",
    "data = {k: np.ma.array(v, mask=flags[k], fill_value=np.nan) for k, v \\\n",
    "        in data.items()}\n",
    "mdata = np.ma.empty((hd.Nfreqs, hd.Ntimes, hd.Nbls), fill_value=np.nan, \\\n",
    "                     dtype=complex)\n",
    "for i, bl in enumerate(flat_bls):\n",
    "    mdata[..., i] = data[bl].transpose()\n",
    "    \n",
    "data = mdata.filled() # dimensions (freqs, times, bls)\n",
    "flags = mdata.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb601d",
   "metadata": {},
   "source": [
    "### Redundant averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b775ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_bls = reds[0]\n",
    "slct_bl_idxs = np.array([bl_dict[slct_bl] for slct_bl in slct_bls])\n",
    "slct_data = data[..., slct_bl_idxs]\n",
    "slct_flags = flags[..., slct_bl_idxs]\n",
    "assert slct_flags.sum() == np.isnan(slct_data).sum()\n",
    "print('Looking at baselines redundant to {}'.format(slct_bls[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cad2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one time integration / frequency slice with high variance\n",
    "idxs = np.unravel_index(np.nanargmax(np.nanstd(slct_data, axis=-1)), \\\n",
    "                        slct_data.shape[:2])\n",
    "print('Selecting freq / time slice {}'.format(idxs))\n",
    "slct_data_slice = slct_data[idxs[0], idxs[1], :]\n",
    "\n",
    "flt_nan = lambda x: x[~np.isnan(x)]\n",
    "sample_gmean = geometric_mean(flt_nan(slct_data_slice))\n",
    "sample_gmed = geometric_median(slct_data_slice, keep_res=True)\n",
    "sample_tmed = tukey_median(slct_data_slice)['barycenter']\n",
    "sample_mmed = Cmardia_median(slct_data_slice)\n",
    "bad_med = lambda x : np.nanmedian(x.real) + np.nanmedian(x.imag)*1j\n",
    "sample_bmed = bad_med(slct_data_slice)\n",
    "sample_hmean = rsc_mean(slct_data_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f25452",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_ests = list(zip([sample_gmean, sample_gmed, sample_tmed, sample_mmed, sample_bmed, sample_hmean], \n",
    "               ['Geometric Mean', 'Geometric Median', 'Tukey Median', 'Mardia Median', \\\n",
    "                'Separate Median', 'HERA Mean']))\n",
    "for me in med_ests:\n",
    "    print('{:17s}: {:4f}'.format(me[1], me[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6), dpi=100)\n",
    "\n",
    "ax.scatter(slct_data_slice.real, slct_data_slice.imag, alpha=0.5)\n",
    "ax.plot(sample_gmean.real, sample_gmean.imag, 'ro', label='Geo mean')\n",
    "ax.plot(sample_gmed.real, sample_gmed.imag, 'co', label='Geo med')\n",
    "ax.plot(sample_tmed.real, sample_tmed.imag, 'yo', label='Tukey')\n",
    "ax.plot(sample_mmed.real, sample_mmed.imag, 'ko', label='Mardia')\n",
    "ax.plot(sample_bmed.real, sample_bmed.imag, 'bo', label='Separate')\n",
    "ax.plot(sample_hmean.real, sample_hmean.imag, 'go', label='HERA')\n",
    "\n",
    "ax.annotate(slct_bls[0], xy=(0.05, 0.05), xycoords='axes fraction')\n",
    "ax.set_xlabel(r'$\\mathfrak{Re} \\; (V)$')\n",
    "ax.set_ylabel(r'$\\mathfrak{Im}(V)$')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_int = np.where(~np.isnan(data).all(axis=(0, 2)))[0][0] # first non-nan index\n",
    "# perhaps find index with fewest nans?\n",
    "gmean_res = np.empty((data.shape[0], len(reds)), dtype=complex)\n",
    "gmed_res, tmed_res, mmed_res, bmed_res, hmean_res = \\\n",
    "    [np.empty_like(gmean_res) for _ in range(5)]\n",
    "\n",
    "gmed_bf, mmed_bf = None, None\n",
    "for bl, bl_grp in enumerate(reds):\n",
    "    slct_bl_idxs = np.array([bl_dict[slct_bl] for slct_bl in bl_grp])\n",
    "    for f, frow in enumerate(data[:, time_int, slct_bl_idxs]):\n",
    "        if np.isnan(frow).all():\n",
    "            gmean_ij = gmed_ij = tmed_ij = mmed_ij = bmed_ij = hmean_ij = np.nan\n",
    "        else:\n",
    "            gmean_bf = geometric_mean(flt_nan(frow))\n",
    "            gmed_bf = geometric_median(frow, init_guess=gmed_bf, keep_res=True)\n",
    "            tmed_bf = tukey_median(frow)['barycenter']\n",
    "            mmed_bf = Cmardia_median(frow, init_guess=None)\n",
    "            bmed_bf = bad_med(frow)\n",
    "            hmean_bf = rsc_mean(frow)\n",
    "        gmean_res[f, bl] = gmean_bf\n",
    "        gmed_res[f, bl] = gmed_bf\n",
    "        tmed_res[f, bl] = tmed_bf\n",
    "        mmed_res[f, bl] = mmed_bf\n",
    "        bmed_res[f, bl] = bmed_bf\n",
    "        hmean_res[f, bl] = hmean_bf\n",
    "        \n",
    "med_est_res = list(zip([i[1] for i in med_ests], \\\n",
    "                  [gmean_res, gmed_res, tmed_res, mmed_res, bmed_res, hmean_res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(10, 20), dpi=100)\n",
    "spec = gridspec.GridSpec(nrows=2*len(med_ests), figure=fig, ncols=2)\n",
    "\n",
    "axes = []\n",
    "for i in range(len(med_ests)):\n",
    "    ax1 = fig.add_subplot(spec[i*2:2+i*2, 0])\n",
    "    ax2 = fig.add_subplot(spec[i*2, 1])\n",
    "    ax3 = fig.add_subplot(spec[i*2+1, 1])\n",
    "    axes.append([ax1, ax2, ax3])\n",
    "\n",
    "color = [None for i in med_est_res]\n",
    "for m, med_est in enumerate(med_est_res):\n",
    "    for i, bl_grp in enumerate(range(len(reds))):\n",
    "        axes[m][0].plot(hd.freqs, med_est[1][:, i].real, color=color[m], \\\n",
    "                   label='{}'.format(reds[i][0]) + r' $\\mathfrak{Re}$')\n",
    "        c = axes[m][0].get_lines()[-1].get_color()\n",
    "        color[m] = next(axes[m][0]._get_lines.prop_cycler)['color']\n",
    "        axes[m][0].plot(hd.freqs, med_est[1][:, i].imag, color=c, \\\n",
    "                   label='{}'.format(reds[i][0]) + r' $\\mathfrak{Im}$', ls='--')\n",
    "        axes[m][1].plot(hd.freqs, np.abs(med_est[1][:, i]), color=c, \\\n",
    "                   label='{}'.format(reds[i][0]) + r' $|V|$')\n",
    "        axes[m][2].plot(hd.freqs, np.angle(med_est[1][:, i]), color=c, \\\n",
    "                   label='{}'.format(reds[i][0]) + r' $\\varphi$', ls='--')\n",
    "        axes[m][0].text(x=0.05, y=0.5, s=med_est[0], transform=axes[m][0].transAxes, \\\n",
    "                        fontsize=10, style='normal', weight='light')\n",
    "\n",
    "for ax in axes:\n",
    "    ax[0].set_ylabel(r'$V$')\n",
    "    \n",
    "for ax in axes[-1]:\n",
    "    ax.set_xlabel(r'$\\nu$')\n",
    "    \n",
    "axes[0][0].set_title('Cartesian')\n",
    "axes[0][1].set_title('Polar')\n",
    "\n",
    "for ax in axes[0]:\n",
    "    ax.legend(framealpha=0.5, loc=1)\n",
    "\n",
    "plt.suptitle('Median estimates for 14-m EW baselines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820193a",
   "metadata": {},
   "source": [
    "### LST averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4693d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_xd_data = np.load(os.path.join(DATAPATH, 'xd_vis.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a809ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd_data = sample_xd_data['data'] # dimensions (days, freqs, times, bls)\n",
    "xd_flags = sample_xd_data['flags']\n",
    "xd_data[xd_flags] = np.nan\n",
    "\n",
    "xd_redg = sample_xd_data['redg']\n",
    "xd_times = sample_xd_data['times']\n",
    "xd_freqs = sample_xd_data['chans']\n",
    "xd_days = sample_xd_data['days']\n",
    "xd_pol = sample_xd_data['pol'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_grp = 0 # only look at 0th baseline group\n",
    "\n",
    "slct_bl_idxs = np.where(xd_redg[:, 0] == bl_grp)[0]\n",
    "data = xd_data[..., slct_bl_idxs]\n",
    "flags = xd_flags[..., slct_bl_idxs]\n",
    "slct_red_bl = xd_redg[slct_bl_idxs[0], :][1:]\n",
    "print('Looking at baselines redundant to ({}, {}, \\'{}\\')'.\\\n",
    "      format(*slct_red_bl, xd_pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90852c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_bls = 4 # just pick the first four baselines from the selected baseline group\n",
    "\n",
    "# use 2 time integrations for each median, as done in HERA LST-binning\n",
    "new_no_tints = int(np.ceil(data.shape[2]/2))\n",
    "xd_gmed_res_t = np.empty((xd_data.shape[1], new_no_tints, no_bls), dtype=complex)\n",
    "xd_tmed_res_t, xd_bmed_res_t, xd_hmean_res_t = \\\n",
    "    [np.empty_like(xd_gmed_res_t) for _ in range(3)]\n",
    "\n",
    "gmed_ij = None\n",
    "for bl in range(no_bls):\n",
    "    xd_data_b = data[..., bl]\n",
    "    for freq in range(xd_data_b.shape[1]):\n",
    "        for tint in range(new_no_tints):\n",
    "            xd_data_bft = xd_data_b[:, freq, 2*tint:2*tint+2].flatten()\n",
    "            if np.isnan(xd_data_bft).all():\n",
    "                gmed_ft = tmed_ft = bmed_ft = hmean_ft = np.nan\n",
    "            else:\n",
    "                gmed_ft = geometric_median(xd_data_bft, init_guess=gmed_ij, \\\n",
    "                                           keep_res=True)\n",
    "                tmed_ft = tukey_median(xd_data_bft)['barycenter']\n",
    "                bmed_ft = bad_med(xd_data_bft)\n",
    "                hmean_ft = rsc_mean(xd_data_bft)\n",
    "            xd_gmed_res_t[freq, tint, bl] = gmed_ft\n",
    "            xd_tmed_res_t[freq, tint, bl] = tmed_ft\n",
    "            xd_bmed_res_t[freq, tint, bl] = bmed_ft\n",
    "            xd_hmean_res_t[freq, tint, bl] = hmean_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa855e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = [xd_gmed_res_t, xd_tmed_res_t, xd_bmed_res_t, xd_hmean_res_t]\n",
    "flt_arrs = []\n",
    "for arr in arrs:\n",
    "    nan_bl = np.isnan(arr).all(axis=(0, 1))\n",
    "    if nan_bl.any():\n",
    "        arr = np.delete(arr, np.where(nan_bl)[0], axis=-1)\n",
    "    flt_arrs.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692208fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_arrs = [[arr[..., i] for i in range(flt_arrs[0].shape[-1])] for arr in flt_arrs]\n",
    "titles = ['Geometric Median', 'Tukey Median', 'Separate Median', 'HERA Mean']\n",
    "\n",
    "grid_heatmaps(grid_arrs, apply_np_fn='abs', titles=titles, ybase=25, \\\n",
    "              ylabels=reds[bl_grp][:no_bls], figsize=(12, 10), clip_pctile=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4bd05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_heatmaps(grid_arrs, apply_np_fn='angle', titles=titles, ybase=25, \\\n",
    "              ylabels=reds[bl_grp][:no_bls], figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_heatmaps(grid_arrs, apply_np_fn='real', titles=titles, ybase=25, \\\n",
    "              ylabels=reds[bl_grp][:no_bls], figsize=(12, 10), clip_pctile=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_heatmaps(grid_arrs, apply_np_fn='imag', titles=titles, ybase=25, \\\n",
    "              ylabels=reds[bl_grp][:no_bls], figsize=(12, 10), clip_pctile=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc010a9",
   "metadata": {},
   "source": [
    "### LST + redundant averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at 2 consecutive time integrations / 1 frequency slice with high variance\n",
    "idxs = np.unravel_index(np.nanargmax(np.nanstd(data[..., :-1, :], axis=(0, -1))), \\\n",
    "                        data.shape[1:-1])\n",
    "print('Selecting freq / %time slice: ({}, {}-{})'.format(idxs[0], idxs[1], idxs[1]+1))\n",
    "\n",
    "# Have visibilities across days for the same baseline - can flatten\n",
    "# the data array and perform statistics on the whole dataset\n",
    "data_slice = data[:, idxs[0], idxs[1]:idxs[1]+2, :].flatten()\n",
    "\n",
    "xd_sample_gmean = geometric_mean(flt_nan(data_slice))\n",
    "xd_sample_gmed = geometric_median(data_slice, keep_res=True)\n",
    "xd_sample_tmed = tukey_median(data_slice)['barycenter']\n",
    "xd_sample_mmed = Cmardia_median(data_slice)\n",
    "xd_sample_bmed = bad_med(data_slice)\n",
    "xd_sample_hmean = rsc_mean(data_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f05842",
   "metadata": {},
   "source": [
    "Alternatively, we could take the median of the visibility amplitude and the Mardia median of the phase. While this is an improvement on doing the median on cartesian coordinates separately, it still does not wholly consider the complex data. The geometric median or the Tukey median would be preferable methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_ests = list(zip([xd_sample_gmean, xd_sample_gmed, xd_sample_tmed, xd_sample_mmed, \\\n",
    "                     xd_sample_bmed, xd_sample_hmean], \\\n",
    "               ['Geometric Mean', 'Geometric Median', 'Tukey Median', 'Mardia Median', \\\n",
    "                'Separate Median', 'HERA Mean'], \\\n",
    "               ['ro', 'co', 'yo', 'ko', 'bo', 'go']))\n",
    "for me in med_ests:\n",
    "    print('{:17s}: {:4f}'.format(me[1], me[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fad85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.scatter(flt_nan(data_slice).real, flt_nan(data_slice).imag, alpha=0.5)\n",
    "for i, med_est in enumerate(med_ests):\n",
    "    ax.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "\n",
    "# zoomed in sub region of the original image\n",
    "axins = zoomed_inset_axes(ax, zoom=6, loc=4)\n",
    "axins.scatter(flt_nan(data_slice).real, flt_nan(data_slice).imag, alpha=0.5)\n",
    "for i, med_est in enumerate(med_ests):\n",
    "    axins.plot(med_est[0].real, med_est[0].imag, med_est[2])\n",
    "\n",
    "x1 = np.floor(np.min([i[0].real for i in med_ests[:-2]]))\n",
    "x2 = np.ceil(np.max([i[0].real for i in med_ests[:-2]]))\n",
    "y1 = np.floor(np.min([i[0].imag for i in med_ests[:-2]]))\n",
    "y2 = np.ceil(np.max([i[0].imag for i in med_ests[:-2]]))\n",
    "axins.set_xlim(x1, x2)\n",
    "axins.set_ylim(y1, y2)\n",
    "\n",
    "axins.tick_params(axis='x', direction='in', pad=-15)\n",
    "mark_inset(ax, axins, loc1=1, loc2=3, fc='none', ec='0.5')\n",
    "\n",
    "ax.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "            xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'))\n",
    "ax.set_xlabel(r'$\\mathfrak{Re} \\; (V)$')\n",
    "ax.set_ylabel(r'$\\mathfrak{Im} \\; (V)$')\n",
    "ax.set_title(textwrap.fill('Bivariate location estimators for redundant '\\\n",
    "    'visibilities aggregated across JDs', 60))\n",
    "\n",
    "ax.legend(loc=1, prop={'size': 8})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a3965",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=flt_nan(data_slice).real, y=flt_nan(data_slice).imag, \\\n",
    "                  kind='kde', height=8, cmap='Blues', fill=True, space=0)\n",
    "g.set_axis_labels(r'$\\mathfrak{Re} \\; (V)$', r'$\\mathfrak{Im} \\; (V)$', size=14)\n",
    "for i, med_est in enumerate(med_ests):\n",
    "    g.ax_joint.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "legend_properties = {'size': 10}\n",
    "g.ax_joint.legend(prop=legend_properties, loc='upper right')\n",
    "g.ax_joint.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "    xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "    size=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bec739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 2 time integrations for each median, as done in HERA LST-binning\n",
    "new_no_tints = int(np.ceil(xd_data.shape[2]/2))\n",
    "xd_gmed_res = np.empty((xd_data.shape[1], new_no_tints), dtype=complex)\n",
    "xd_tmed_res, xd_bmed_res, xd_hmean_res = [np.empty_like(xd_gmed_res) for _ in range(3)]\n",
    "\n",
    "gmed_ij = None\n",
    "slct_bl_idxs = np.array([bl_dict[slct_bl] for slct_bl in reds[bl_grp]])\n",
    "xd_data_b = xd_data[..., slct_bl_idxs]\n",
    "for freq in range(xd_data_b.shape[1]):\n",
    "    for tint in range(new_no_tints):\n",
    "        xd_data_bft = xd_data_b[:, freq, 2*tint:2*tint+2, :].flatten()\n",
    "        if np.isnan(xd_data_bft).all():\n",
    "            gmed_ft = tmed_ft = bmed_ft = hmean_ft = np.nan\n",
    "        else:\n",
    "            gmed_ft = geometric_median(xd_data_bft, init_guess=gmed_ij, keep_res=True)\n",
    "            tmed_ft = tukey_median(xd_data_bft)['barycenter']\n",
    "            bmed_ft = bad_med(xd_data_bft)\n",
    "            hmean_ft = rsc_mean(xd_data_bft)\n",
    "        xd_gmed_res[freq, tint] = gmed_ft\n",
    "        xd_tmed_res[freq, tint] = tmed_ft\n",
    "        xd_bmed_res[freq, tint] = bmed_ft\n",
    "        xd_hmean_res[freq, tint] = hmean_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ad096",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = [xd_gmed_res, xd_tmed_res, xd_bmed_res, xd_hmean_res]\n",
    "\n",
    "tr_arrs = lambda x, np_fn: [getattr(np, np_fn)(i) for i in x]\n",
    "garrs = [tr_arrs(arrs, 'abs'), tr_arrs(arrs, 'angle'), tr_arrs(arrs, 'real'), tr_arrs(arrs, 'imag')]\n",
    "garrs = [[arr[i] for arr in garrs] for i in range(len(garrs[0]))]\n",
    "\n",
    "titles = ['Geometric Median', 'Tukey Median', 'Separate Median', 'HERA Mean']\n",
    "ylabels = ['Amp', 'Phase', r'$\\mathfrak{Re}$', r'$\\mathfrak{Im}$']\n",
    "\n",
    "grid_heatmaps(garrs, titles=titles, figsize=(12, 10), ylabels=ylabels, ybase=25, clip_pctile=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3a149",
   "metadata": {},
   "source": [
    "#### Smoothness of median results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e7f704",
   "metadata": {},
   "source": [
    "Calculate standard deviation of the distances between successive points in either frequency or time to get an idea of the smootheness of the location results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75dcef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in time\n",
    "t_smoothness = []\n",
    "for arr in arrs:\n",
    "    t_stds = np.empty(arr.shape[0])\n",
    "    for f in range(arr.shape[0]):\n",
    "        dists = np.abs(np.ediff1d(arr[f, :]))\n",
    "        t_stds[f] = np.nanstd(dists)\n",
    "    t_smoothness.append(np.nanmean(t_stds))\n",
    "print('Smootheness in time: \\n{}\\n{}\\n'.format(titles, t_smoothness))\n",
    "\n",
    "# in frequency\n",
    "f_smoothness = []\n",
    "for arr in arrs:\n",
    "    f_stds = np.empty(arr.shape[1])\n",
    "    for t in range(arr.shape[1]):\n",
    "        dists = np.abs(np.ediff1d(arr[:, t]))\n",
    "        f_stds[t] = np.nanstd(dists)\n",
    "    f_smoothness.append(np.nanmean(f_stds))\n",
    "print('Smootheness in frequency: \\n{}\\n{}'.format(titles, f_smoothness))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a81e4",
   "metadata": {},
   "source": [
    "#### Biggest difference in geometric median and HERA mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_slice = np.mean(flags, axis=(0, 3))\n",
    "even_end = int(np.floor(ok_slice.shape[1]/2))*2\n",
    "ok_slice_ = 0.5 * (ok_slice[:, :even_end:2] + ok_slice[:, 1:even_end:2]) # since 2 tints are used\n",
    "if even_end != flags.shape[2]:\n",
    "    ok_slice_ = np.append(ok_slice_, np.expand_dims(ok_slice[:, -1], 1), axis=1)\n",
    "ok_slice = ok_slice_ < 0.5 # only if less than 50% of flags are flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa00ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_idx = np.unravel_index(np.nanargmax(np.abs(xd_gmed_res[ok_slice] - xd_hmean_res[ok_slice])), \\\n",
    "                          xd_gmed_res.shape)\n",
    "print('Frequency/time slice {} shows strong deviation between the geometric median and the '\\\n",
    "      'HERA mean.'.format(bd_idx))\n",
    "bd_data = xd_data_b[:, bd_idx[0], 2*bd_idx[1]:2*bd_idx[1]+2, :].flatten()\n",
    "\n",
    "bd_med_ests = list(zip([xd_gmed_res[bd_idx], xd_hmean_res[bd_idx]], \\\n",
    "                       ['Geometric Median', 'HERA Mean'], \\\n",
    "                       ['ro', 'go']))\n",
    "\n",
    "g = sns.jointplot(x=flt_nan(bd_data).real, y=flt_nan(bd_data).imag, \\\n",
    "                  kind='kde', height=8, cmap='Blues', fill=True, space=0)\n",
    "g.set_axis_labels(r'$\\mathfrak{Re} \\; (V)$', r'$\\mathfrak{Im} \\; (V)$', size=14)\n",
    "for i, med_est in enumerate(bd_med_ests):\n",
    "    g.ax_joint.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "legend_properties = {'size': 10}\n",
    "g.ax_joint.legend(prop=legend_properties, loc='upper right')\n",
    "g.ax_joint.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "    xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "    size=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c141f87",
   "metadata": {},
   "source": [
    "### Test of normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1594632",
   "metadata": {},
   "source": [
    "#### Shapiro-Wilk test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d5ce6",
   "metadata": {},
   "source": [
    "We test the aggregated visibility data (over days, redundant baselines and consecutive time integrations) for normality using the Shapiro-Wilk test, to see if the data is Gaussian distributed for the $\\mathfrak{Re}$ and $\\mathfrak{Im}$ components separately, thus justifying the use of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21152f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_w_re = np.empty_like(xd_gmed_res, dtype=float)\n",
    "shapiro_w_im, shapiro_p_re, shapiro_p_im = [np.empty_like(shapiro_w_re) for _ in range(3)]\n",
    "for freq in range(xd_data_b.shape[1]):\n",
    "    for tint in range(new_no_tints):\n",
    "        xd_data_bft = flt_nan(xd_data_b[:, freq, 2*tint:2*tint+2, :].flatten())\n",
    "        if np.isnan(xd_data_bft).all():\n",
    "            re_shapiro_stat = im_shapiro_stat = re_shapiro_pval = re_shapiro_pval = np.nan\n",
    "        else:\n",
    "            re_shapiro = shapiro(xd_data_bft.real)\n",
    "            im_shapiro = shapiro(xd_data_bft.imag)\n",
    "            re_shapiro_stat = re_shapiro.statistic\n",
    "            im_shapiro_stat = im_shapiro.statistic\n",
    "            re_shapiro_pval = re_shapiro.pvalue\n",
    "            re_shapiro_pval = im_shapiro.pvalue\n",
    "        \n",
    "        shapiro_w_re[freq, tint] = re_shapiro.statistic\n",
    "        shapiro_w_im[freq, tint] = im_shapiro.statistic\n",
    "        shapiro_p_re[freq, tint] = re_shapiro.pvalue\n",
    "        shapiro_p_im[freq, tint] = im_shapiro.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ef1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [[r'$W \\; \\mathrm{statistic} \\; - \\; \\mathfrak{Re}(V)$', \\\n",
    "          r'$p \\; \\mathrm{value} \\; - \\; \\mathfrak{Re}(V)$'], \\\n",
    "          [r'$W \\; \\mathrm{statistic} \\; - \\; \\mathfrak{Im}(V)$', \\\n",
    "          r'$p \\; \\mathrm{value} \\; - \\; \\mathfrak{Im}(V)$']]\n",
    "grid_heatmaps([[shapiro_w_re, shapiro_p_re], [shapiro_w_im, shapiro_p_im]], \\\n",
    "             titles=titles, figsize=(14, 7), ybase=25, share_cbar=True, clip_pctile=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0591bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example histograms for aggregated visibility data\n",
    "\n",
    "# picking frequency/time slice with worst shapiro statistic for Re visibilities\n",
    "re_shap_min = np.unravel_index(np.nanargmin(shapiro_p_re), shapiro_w_re.shape)\n",
    "print('Slice {} has Shapiro-Wilk test p value {:.5f} for the Re component.\\n'\\\n",
    "      .format(re_shap_min, shapiro_p_re[re_shap_min]))\n",
    "\n",
    "print('If the p value < the chosen alpha level (usually taken to be 0.05), then the null hypothesis '\\\n",
    "      'is rejected and there is evidence that the data tested are not normally distributed')\n",
    "\n",
    "hist_data = flt_nan(xd_data_b[:, re_shap_min[0], re_shap_min[1], :])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(14, 7))\n",
    "\n",
    "sns.histplot(hist_data.real, ax=ax[0], binwidth=2.5, kde=True)\n",
    "sns.histplot(hist_data.imag, ax=ax[1], binwidth=2.5, kde=True)\n",
    "\n",
    "ax[0].set_title(r'$\\mathfrak{Re}(V)$')\n",
    "ax[1].set_title(r'$\\mathfrak{Im}(V)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb612d7",
   "metadata": {},
   "source": [
    "#### Henze-Zirkler multivariate normality test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e4e02",
   "metadata": {},
   "source": [
    "We use the HZ test as this considers the entirety of the data. Note that many alternatives tests also exist and that a single statistic does not definitely conclude if the multivariate data is normality distributed or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAD-clipping about Re and Im separately, like HERA\n",
    "nan_flags = np.isnan(xd_data_b)\n",
    "re_clip_f = mad_clip(xd_data_b.real, axis=(0, 3), flags=nan_flags, verbose=True)[1]\n",
    "im_clip_f = mad_clip(xd_data_b.imag, axis=(0, 3), flags=nan_flags, verbose=True)[1]\n",
    "\n",
    "xd_data_bc = xd_data_b.copy()\n",
    "xd_data_bc[re_clip_f + im_clip_f] *= np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "hz_r = np.empty_like(shapiro_w_re)\n",
    "hz_p = np.empty_like(hz_r)\n",
    "hz_n = np.empty_like(hz_r, dtype=bool)\n",
    "\n",
    "hz_r_c = np.empty_like(hz_r)\n",
    "hz_p_c = np.empty_like(hz_r)\n",
    "hz_n_c = np.empty_like(hz_n)\n",
    "\n",
    "bool_dict = {'NO': False, 'YES': True, np.nan: False}\n",
    "\n",
    "for freq in range(xd_data_b.shape[1]):\n",
    "    for tint in range(new_no_tints):\n",
    "        xd_data_bft = flt_nan(xd_data_b[:, freq, 2*tint:2*tint+2, :].flatten())\n",
    "        xd_data_bcft = flt_nan(xd_data_bc[:, freq, 2*tint:2*tint+2, :].flatten())\n",
    "        \n",
    "        hz_res = mv_normality(xd_data_bft, method='hz')\n",
    "        hz_r[freq, tint] = hz_res['HZ']\n",
    "        hz_p[freq, tint] = hz_res['p value']\n",
    "        hz_n[freq, tint] = bool_dict[hz_res['MVN']]\n",
    "        \n",
    "        hz_res_c = mv_normality(xd_data_bcft, method='hz')\n",
    "        hz_r_c[freq, tint] = hz_res_c['HZ']\n",
    "        hz_p_c[freq, tint] = hz_res_c['p value']\n",
    "        hz_n_c[freq, tint] = bool_dict[hz_res_c['MVN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fe331",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [r'$HZ \\; \\mathrm{statistic}$', r'$p \\; \\mathrm{value}$', 'Normality']\n",
    "row_heatmaps([hz_r, hz_p, hz_n], titles=titles, figsize=(14, 7), share_cbar=False, \\\n",
    "             cbar_loc=None, clip_pctile=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAD-clipped data\n",
    "titles = [r'$HZ \\; \\mathrm{statistic}$', r'$p \\; \\mathrm{value}$', 'Normality']\n",
    "row_heatmaps([hz_r_c, hz_p_c, hz_n_c], titles=titles, figsize=(14, 7), share_cbar=False, \\\n",
    "             cbar_loc=None, clip_pctile=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fa77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking frequency/time slice with worst HZ statistic\n",
    "hz_p_min = np.unravel_index(np.nanargmin(hz_p), hz_p.shape)\n",
    "print('Slice {} has HZ test p value {:.5f}.\\n'\\\n",
    "      .format(hz_p_min, hz_p[hz_p_min]))\n",
    "\n",
    "print('If the p value < the chosen alpha level (usually taken to be 0.05), then the null hypothesis '\\\n",
    "      'is rejected and there is evidence that the data tested are not normally distributed')\n",
    "\n",
    "hz_data = flt_nan(xd_data_b[:, hz_p_min[0], hz_p_min[1], :])\n",
    "\n",
    "g = sns.jointplot(x=hz_data.real, y=hz_data.imag, \\\n",
    "                  kind='kde', height=8, cmap='Blues', fill=True, space=0)\n",
    "g.set_axis_labels(r'$\\mathfrak{Re} \\; (V)$', r'$\\mathfrak{Im} \\; (V)$', size=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
