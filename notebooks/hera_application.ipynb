{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e795ca4e",
   "metadata": {},
   "source": [
    "<center><strong><font size=+3>Applications of robust 2D median estimators to HERA data</font></center>\n",
    "<br><br>\n",
    "</center>\n",
    "<center><strong><font size=+2>Matyas Molnar and Bojan Nikolic</font><br></strong></center>\n",
    "<br><center><strong><font size=+1>Astrophysics Group, Cavendish Laboratory, University of Cambridge</font></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset, zoomed_inset_axes\n",
    "from scipy import signal\n",
    "from scipy.stats import chi2, shapiro\n",
    "from scipy.stats.mstats import gmean as geometric_mean\n",
    "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
    "\n",
    "from hera_cal.io import HERAData\n",
    "from hera_cal.redcal import get_reds\n",
    "\n",
    "from robstat.hera_vis import agg_tint_rephase\n",
    "from robstat.ml import extrem_nans, nan_interp2d\n",
    "from robstat.plotting import grid_heatmaps, SeabornFig2Grid, row_heatmaps\n",
    "from robstat.robstat import c_mardia_median, geometric_median, mardia_median, mv_median, \\\n",
    "mv_normality, mv_outlier, tukey_median\n",
    "from robstat.stdstat import mad_clip, rsc_mean\n",
    "from robstat.utils import DATAPATH, flt_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn on multiprocessing\n",
    "mp = True\n",
    "import multiprocess as multiprocessing\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','serif':['cm']})\n",
    "rc('text', usetex=True)\n",
    "rc('text.latex', preamble=r'\\usepackage{amssymb} \\usepackage{amsmath}')\n",
    "\n",
    "plot_figs = False\n",
    "if plot_figs:\n",
    "    mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb49aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "mad_sigma = 5.0 # sigma threshold for MAD-clipping, default is 5\n",
    "no_bins_agg = 2 # averaging over n consecutive time bins in LST averaging\n",
    "# (2 by default, like in HERA analysis pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d057f5",
   "metadata": {},
   "source": [
    "### Load HERA visibility data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = os.path.join(DATAPATH, 'zen.2458098.43869.HH.OCRSA.uvh5')\n",
    "\n",
    "hd = HERAData(sample_data)\n",
    "data, flags, _ = hd.read()\n",
    "\n",
    "reds = get_reds(hd.antpos, pols=hd.pols)\n",
    "flat_bls = [bl for grp in reds for bl in grp if bl in data.keys()]\n",
    "reds = [grp for grp in reds if set(grp).issubset(flat_bls)]\n",
    "bl_dict = {k: i for i, k in enumerate(flat_bls)}\n",
    "\n",
    "data = {k: np.ma.array(v, mask=flags[k], fill_value=np.nan) for k, v \\\n",
    "        in data.items()}\n",
    "mdata = np.ma.empty((hd.Nfreqs, hd.Ntimes, hd.Nbls), fill_value=np.nan, \\\n",
    "                     dtype=complex)\n",
    "for i, bl in enumerate(flat_bls):\n",
    "    mdata[..., i] = data[bl].transpose()\n",
    "\n",
    "data = mdata.filled() # dimensions (freqs, times, bls)\n",
    "flags = mdata.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb601d",
   "metadata": {},
   "source": [
    "### Redundant averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b775ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_bls = reds[0]\n",
    "slct_bl_idxs = np.array([bl_dict[slct_bl] for slct_bl in slct_bls])\n",
    "slct_data = data[..., slct_bl_idxs]\n",
    "slct_flags = flags[..., slct_bl_idxs]\n",
    "assert slct_flags.sum() == np.isnan(slct_data).sum()\n",
    "print('Looking at baselines redundant to {}'.format(slct_bls[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cad2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one time integration / frequency slice with high variance\n",
    "idxs = np.unravel_index(np.nanargmax(np.nanstd(slct_data, axis=-1)), \\\n",
    "                        slct_data.shape[:2])\n",
    "print('Selecting freq / time slice {}'.format(idxs))\n",
    "slct_data_slice = slct_data[idxs[0], idxs[1], :]\n",
    "\n",
    "sample_gmean = geometric_mean(flt_nan(slct_data_slice))\n",
    "sample_gmed = geometric_median(slct_data_slice, options=dict(keep_res=True))\n",
    "sample_tmed = tukey_median(slct_data_slice)['barycenter']\n",
    "sample_mmed = c_mardia_median(slct_data_slice)\n",
    "marg_med = lambda x : np.nanmedian(x.real) + np.nanmedian(x.imag)*1j\n",
    "sample_bmed = marg_med(slct_data_slice)\n",
    "sample_hmean = rsc_mean(slct_data_slice, sigma=mad_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f25452",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_ests = list(zip([sample_gmean, sample_gmed, sample_tmed, sample_mmed, sample_bmed, sample_hmean], \n",
    "               ['Geometric Mean', 'Geometric Median', 'Tukey Median', 'Mardia Median', \\\n",
    "                'Marginal Median', 'HERA Mean']))\n",
    "for me in med_ests:\n",
    "    print('{:17s}: {:4f}'.format(me[1], me[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_label = r'$\\mathfrak{Re}(V)$'\n",
    "im_label = r'$\\mathfrak{Im}(V)$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "ax.scatter(slct_data_slice.real, slct_data_slice.imag, alpha=0.5)\n",
    "ax.plot(sample_gmean.real, sample_gmean.imag, 'co', label='Geometric Mean')\n",
    "ax.plot(sample_gmed.real, sample_gmed.imag, 'ro', label='Geometric Median')\n",
    "ax.plot(sample_tmed.real, sample_tmed.imag, 'yo', label='Tukey Median')\n",
    "ax.plot(sample_mmed.real, sample_mmed.imag, 'ko', label='Mardia Median')\n",
    "ax.plot(sample_bmed.real, sample_bmed.imag, 'bo', label='Marginal Median')\n",
    "ax.plot(sample_hmean.real, sample_hmean.imag, 'go', label='HERA Mean')\n",
    "\n",
    "ax.annotate(slct_bls[0], xy=(0.05, 0.05), xycoords='axes fraction', \\\n",
    "            bbox=dict(boxstyle='round', facecolor='white'), size=12)\n",
    "ax.annotate('Freq: {:.2f} MHz, LST: {:.3f}'.format(hd.freqs[idxs[0]]/1e6, hd.lsts[idxs[1]]*12/np.pi), \\\n",
    "            xy=(0.05, 0.95), xycoords='axes fraction', \\\n",
    "            bbox=dict(boxstyle='round', facecolor='white'), size=12)\n",
    "\n",
    "ax.set_xlabel(re_label)\n",
    "ax.set_ylabel(im_label)\n",
    "\n",
    "plt.legend(loc='lower right', prop={'size': 10})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = os.path.join(DATAPATH, 'loc_res')\n",
    "if not os.path.exists(res_dir):\n",
    "    os.mkdir(res_dir)\n",
    "    \n",
    "red_res_fn = os.path.join(res_dir, os.path.basename(sample_data).replace('.uvh5', '.res.npz'))\n",
    "if not os.path.exists(red_res_fn):\n",
    "\n",
    "    time_int = np.where(~np.isnan(data).all(axis=(0, 2)))[0][0] # first non-nan index\n",
    "\n",
    "    gmean_res = np.empty((hd.Nfreqs, len(reds)), dtype=complex)\n",
    "    gmed_res, tmed_res, mmed_res, bmed_res, hmean_res = \\\n",
    "        [np.empty_like(gmean_res) for _ in range(5)]\n",
    "\n",
    "    gmed_bf_init = None\n",
    "    for bl, bl_grp in enumerate(reds):\n",
    "        slct_bl_idxs = np.array([bl_dict[slct_bl] for slct_bl in bl_grp])\n",
    "        for f, frow in enumerate(data[:, time_int, slct_bl_idxs]):\n",
    "            if np.isnan(frow).all():\n",
    "                gmean_bf = gmed_bf = tmed_bf = mmed_bf = bmed_bf = hmean_bf = np.nan + 1j*np.nan\n",
    "            else:\n",
    "                gmean_bf = geometric_mean(flt_nan(frow))\n",
    "                gmed_bf = geometric_median(frow, init_guess=gmed_bf_init, options=dict(keep_res=True))\n",
    "                gmed_bf_init = gmed_bf\n",
    "                tmed_bf = tukey_median(frow)['barycenter']\n",
    "                mmed_bf = c_mardia_median(frow, init_guess=None)\n",
    "                bmed_bf = marg_med(frow)\n",
    "                hmean_bf = rsc_mean(frow, sigma=mad_sigma)\n",
    "            gmean_res[f, bl] = gmean_bf\n",
    "            gmed_res[f, bl] = gmed_bf\n",
    "            tmed_res[f, bl] = tmed_bf\n",
    "            mmed_res[f, bl] = mmed_bf\n",
    "            bmed_res[f, bl] = bmed_bf\n",
    "            hmean_res[f, bl] = hmean_bf\n",
    "            \n",
    "    np.savez(red_res_fn, gmean_res=gmean_res, gmed_res=gmed_res, tmed_res=tmed_res, \\\n",
    "             mmed_res=mmed_res, bmed_res=bmed_res, hmean_res=hmean_res)\n",
    "\n",
    "else:\n",
    "    red_res = np.load(red_res_fn)\n",
    "    gmean_res = red_res['gmean_res']\n",
    "    gmed_res = red_res['gmed_res']\n",
    "    tmed_res = red_res['tmed_res']\n",
    "    mmed_res = red_res['mmed_res']\n",
    "    bmed_res = red_res['bmed_res']\n",
    "    hmean_res = red_res['hmean_res']\n",
    "        \n",
    "med_est_res = list(zip([i[1] for i in med_ests], \\\n",
    "    [gmean_res, gmed_res, tmed_res, mmed_res, bmed_res, hmean_res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(10, 20), dpi=100)\n",
    "spec = gridspec.GridSpec(nrows=2*len(med_ests), figure=fig, ncols=2)\n",
    "\n",
    "axes = []\n",
    "for i in range(len(med_ests)):\n",
    "    ax1 = fig.add_subplot(spec[i*2:2+i*2, 0])\n",
    "    ax2 = fig.add_subplot(spec[i*2, 1])\n",
    "    ax3 = fig.add_subplot(spec[i*2+1, 1])\n",
    "    axes.append([ax1, ax2, ax3])\n",
    "\n",
    "color = [None for i in med_est_res]\n",
    "for m, med_est in enumerate(med_est_res):\n",
    "    for i, bl_grp in enumerate(range(len(reds))):\n",
    "        axes[m][0].plot(hd.freqs, med_est[1][:, i].real, color=color[m], \\\n",
    "            label='{}'.format(reds[i][0]) + re_label)\n",
    "        c = axes[m][0].get_lines()[-1].get_color()\n",
    "        color[m] = next(axes[m][0]._get_lines.prop_cycler)['color']\n",
    "        axes[m][0].plot(hd.freqs, med_est[1][:, i].imag, color=c, \\\n",
    "            label='{}'.format(reds[i][0]) + im_label, ls='--')\n",
    "        axes[m][1].plot(hd.freqs, np.abs(med_est[1][:, i]), color=c, \\\n",
    "            label='{}'.format(reds[i][0]))\n",
    "        axes[m][2].plot(hd.freqs, np.angle(med_est[1][:, i]), color=c, \\\n",
    "            label='{}'.format(reds[i][0]), ls='--')\n",
    "        axes[m][0].text(x=0.05, y=0.5, s=med_est[0], transform=axes[m][0].transAxes, \\\n",
    "            fontsize=10, style='normal', weight='light')\n",
    "\n",
    "for ax in axes:\n",
    "    ax[0].set_ylabel(r'$V$')\n",
    "    ax[1].set_ylabel(r'$|V|$')\n",
    "    ax[2].set_ylabel(r'$\\varphi$')\n",
    "    \n",
    "for ax in axes[-1]:\n",
    "    ax.set_xlabel(r'$\\nu$')\n",
    "    \n",
    "axes[0][0].set_title('Cartesian')\n",
    "axes[0][1].set_title('Polar')\n",
    "\n",
    "for ax in axes[0]:\n",
    "    ax.legend(framealpha=0.5, loc=1)\n",
    "\n",
    "plt.suptitle('Median estimates for 14-m EW baselines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820193a",
   "metadata": {},
   "source": [
    "### LST averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4693d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd_vis_file = os.path.join(DATAPATH, 'xd_vis_extd_rph.npz')\n",
    "# xd_vis_file = os.path.join(DATAPATH, 'lstb_no_avg/idr2_lstb_14m_ee_1.40949.npz')\n",
    "sample_xd_data = np.load(xd_vis_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a809ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd_data = sample_xd_data['data']\n",
    "xd_redg = sample_xd_data['redg']\n",
    "xd_pol = sample_xd_data['pol'].item()\n",
    "xd_rad_lsts = sample_xd_data['lsts']\n",
    "xd_hr_lsts = xd_rad_lsts*12/np.pi # in hours\n",
    "JDs = sample_xd_data['JDs']\n",
    "no_days = JDs.size\n",
    "\n",
    "lstb_format = 'lstb_no_avg' in xd_vis_file\n",
    "\n",
    "if lstb_format:\n",
    "    band_1 = [175, 334]\n",
    "    band_2 = [515, 694]\n",
    "\n",
    "    band_i = band_2 # select band here\n",
    "    chans = np.arange(band_i[0], band_i[1]+1)\n",
    "    plt_chans = chans\n",
    "\n",
    "    # data dimensions (2xdays, freqs, times, bls)\n",
    "    xd_data = xd_data[:, chans, ...]\n",
    "    \n",
    "    xd_flags = np.isnan(xd_data)\n",
    "    no_chans = xd_data.shape[1]\n",
    "    freqs = np.linspace(1e8, 2e8, 1025)[:-1][chans]\n",
    "    new_no_tints = xd_data.shape[2]\n",
    "    no_bins_agg = 1\n",
    "    avg_hr_lsts = xd_hr_lsts\n",
    "\n",
    "else:\n",
    "    # data dimensions (days, freqs, times, bls)\n",
    "    xd_flags = sample_xd_data['flags']\n",
    "    xd_data[xd_flags] *= np.nan\n",
    "\n",
    "    xd_times = sample_xd_data['times']\n",
    "    avg_hr_lsts = np.mean(xd_hr_lsts.reshape(-1, no_bins_agg), axis=1)\n",
    "    JDs = sample_xd_data['JDs']\n",
    "\n",
    "    freqs = sample_xd_data['freqs']\n",
    "    chans = sample_xd_data['chans']\n",
    "    if chans[-1]%100 == 99:\n",
    "        plt_chans = np.append(chans, chans[-1]+1)\n",
    "    else:\n",
    "        plt_chans = chans\n",
    "\n",
    "    no_chans = chans.size\n",
    "    no_tints = xd_times.size\n",
    "    new_no_tints = int(np.ceil(no_tints/no_bins_agg))\n",
    "    \n",
    "    no_bins_agg = 2 # averaging over n consecutive time bins in LST averaging\n",
    "\n",
    "    # rephase if averaging over consecutive time bins\n",
    "    if 'rph' in os.path.basename(xd_vis_file) and no_bins_agg > 1:\n",
    "        print('Rephasing visibilities such that every {} rows in time have the same phase centre.'.format(no_bins_agg))\n",
    "        xd_antpos = np.load(xd_vis_file, allow_pickle=True)['antpos'].item()\n",
    "        xd_data = agg_tint_rephase(xd_data, xd_redg, freqs, xd_pol, xd_rad_lsts, xd_antpos, \\\n",
    "                                   no_bins_agg=no_bins_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "myround = lambda x, base=25: base * max(1, round(x/base))\n",
    "tbase = myround(new_no_tints/5, base=10)\n",
    "fbase = myround(no_chans/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_grp = 0 # only look at 0th baseline group\n",
    "\n",
    "slct_bl_idxs = np.where(xd_redg[:, 0] == bl_grp)[0]\n",
    "flags = xd_flags[..., slct_bl_idxs]\n",
    "slct_red_bl = xd_redg[slct_bl_idxs[0], :][1:]\n",
    "xd_data_bls = xd_data[..., slct_bl_idxs]\n",
    "no_bls = slct_bl_idxs.size\n",
    "print('Looking at baselines redundant to ({}, {}, \\'{}\\')'.\\\n",
    "      format(*slct_red_bl, xd_pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90852c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "slct_no_bls = 4 # just pick the first four baselines from the selected baseline group\n",
    "\n",
    "lst_res_fn = os.path.join(res_dir, os.path.basename(xd_vis_file).replace('.npz', '.lst_res.npz'))\n",
    "\n",
    "if not os.path.exists(lst_res_fn):\n",
    "\n",
    "    def freq_iter(freq):\n",
    "        xd_gmed_res_t_f = np.empty((1, new_no_tints, slct_no_bls), dtype=complex)\n",
    "        xd_tmed_res_t_f, xd_bmed_res_t_f, xd_hmean_res_t_f = \\\n",
    "            [np.empty_like(xd_gmed_res_t_f) for _ in range(3)]\n",
    "\n",
    "        gmed_ft_init = None\n",
    "        for bl in range(slct_no_bls):\n",
    "            for tint in range(new_no_tints):\n",
    "                if lstb_format:\n",
    "                    xd_data_bft = xd_data_bls[:, freq, tint, bl].flatten()\n",
    "                else:\n",
    "                    # use no_bins_agg time integrations for each median\n",
    "                    # (2 consecutive ones are used in HERA LST-binning)\n",
    "                    xd_data_bft = xd_data_bls[:, freq, no_bins_agg*tint:no_bins_agg*tint+no_bins_agg, \\\n",
    "                                              bl].flatten()\n",
    "\n",
    "                if np.isnan(xd_data_bft).all():\n",
    "                    gmed_ft = tmed_ft = bmed_ft = hmean_ft = np.nan + 1j*np.nan\n",
    "                else:\n",
    "                    gmed_ft = geometric_median(xd_data_bft, init_guess=gmed_ft_init, \\\n",
    "                                               options=dict(keep_res=True))\n",
    "                    gmed_ft_init = gmed_ft\n",
    "                    tmed_ft = tukey_median(xd_data_bft)['barycenter']\n",
    "                    bmed_ft = marg_med(xd_data_bft)\n",
    "                    hmean_ft = rsc_mean(xd_data_bft, sigma=mad_sigma)\n",
    "\n",
    "                xd_gmed_res_t_f[:, tint, bl] = gmed_ft\n",
    "                xd_tmed_res_t_f[:, tint, bl] = tmed_ft\n",
    "                xd_bmed_res_t_f[:, tint, bl] = bmed_ft\n",
    "                xd_hmean_res_t_f[:, tint, bl] = hmean_ft\n",
    "\n",
    "        return xd_gmed_res_t_f, xd_tmed_res_t_f, xd_bmed_res_t_f, xd_hmean_res_t_f\n",
    "\n",
    "    if mp:\n",
    "        m_pool = multiprocessing.Pool(multiprocessing.cpu_count())\n",
    "        pool_res = m_pool.map(freq_iter, range(no_chans))\n",
    "        m_pool.close()\n",
    "        m_pool.join()\n",
    "    else:\n",
    "        pool_res = list(map(freq_iter, range(no_chans)))\n",
    "\n",
    "    loc_res = np.concatenate(pool_res, axis=1)\n",
    "    xd_gmed_res_t = loc_res[0, ...]\n",
    "    xd_tmed_res_t = loc_res[1, ...]\n",
    "    xd_bmed_res_t = loc_res[2, ...]\n",
    "    xd_hmean_res_t = loc_res[3, ...]\n",
    "\n",
    "    np.savez(lst_res_fn, xd_gmed_res_t=xd_gmed_res_t, xd_tmed_res_t=xd_tmed_res_t, \\\n",
    "             xd_bmed_res_t=xd_bmed_res_t, xd_hmean_res_t=xd_hmean_res_t)\n",
    "\n",
    "else:\n",
    "    lst_res = np.load(lst_res_fn)\n",
    "    xd_gmed_res_t = lst_res['xd_gmed_res_t']\n",
    "    xd_tmed_res_t = lst_res['xd_tmed_res_t']\n",
    "    xd_bmed_res_t = lst_res['xd_bmed_res_t']\n",
    "    xd_hmean_res_t = lst_res['xd_hmean_res_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa855e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = [xd_gmed_res_t, xd_tmed_res_t, xd_bmed_res_t, xd_hmean_res_t]\n",
    "flt_arrs = []\n",
    "for arr in arrs:\n",
    "    nan_bl = np.isnan(arr).all(axis=(0, 1))\n",
    "    if nan_bl.any():\n",
    "        arr = np.delete(arr, np.where(nan_bl)[0], axis=-1)\n",
    "    flt_arrs.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692208fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_arrs = [[arr[..., i] for i in range(flt_arrs[0].shape[-1])] for arr in flt_arrs]\n",
    "titles = ['Geometric Median', 'Tukey Median', 'Marginal Median', 'HERA Mean']\n",
    "\n",
    "ylabels = [str(ylab) + '\\n\\nFrequency Channel' for ylab in reds[bl_grp][:slct_no_bls]]\n",
    "\n",
    "grid_heatmaps(grid_arrs, apply_np_fn='abs', titles=titles, xbase=tbase, ybase=fbase, \\\n",
    "              xlabels='Time bin', ylabels=ylabels, clip_pctile=1, yticklabels=plt_chans, \\\n",
    "              figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4bd05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_heatmaps(grid_arrs, apply_np_fn='angle', titles=titles, xbase=tbase, ybase=fbase, \\\n",
    "              xlabels='Time bin', ylabels=ylabels, yticklabels=plt_chans, figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_heatmaps(grid_arrs, apply_np_fn='real', titles=titles, xbase=tbase, ybase=fbase, \\\n",
    "              xlabels='Time bin', ylabels=ylabels, clip_pctile=1, yticklabels=plt_chans, \\\n",
    "              figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_heatmaps(grid_arrs, apply_np_fn='imag', titles=titles, xbase=tbase, ybase=fbase, \\\n",
    "              xlabels='Time bin', ylabels=ylabels, clip_pctile=1, yticklabels=plt_chans, \\\n",
    "              figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc010a9",
   "metadata": {},
   "source": [
    "### LST + redundant averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at no_bins_agg consecutive time integrations / 1 frequency slice with high variance\n",
    "idxs = np.asarray(np.unravel_index(np.nanargmax(np.nanstd(xd_data_bls[..., \\\n",
    "    :xd_data_bls.shape[2]-no_bins_agg+1, :], axis=(0, -1))), xd_data_bls.shape[1:-1]))\n",
    "\n",
    "idxs = [0, 57]\n",
    "\n",
    "t_adj = idxs[1]%no_bins_agg\n",
    "if t_adj != 0:\n",
    "    idxs[1] -= t_adj\n",
    "    \n",
    "print('Selecting freq / %time slice: ({}, {}-{})'.format(idxs[0], idxs[1], idxs[1]+no_bins_agg-1))\n",
    "\n",
    "# Have visibilities across days for the same baseline (2 time bins)\n",
    "# flatten the data array and perform statistics on the whole dataset\n",
    "if lstb_format:\n",
    "    data_slice = xd_data_bls[:, idxs[0], idxs[1], :].flatten()\n",
    "else:\n",
    "    data_slice = xd_data_bls[:, idxs[0], idxs[1]:idxs[1]+no_bins_agg, :].flatten()\n",
    "\n",
    "xd_sample_gmean = geometric_mean(flt_nan(data_slice))\n",
    "xd_sample_gmed = geometric_median(data_slice, options=dict(keep_res=True))\n",
    "xd_sample_tmed = tukey_median(data_slice)['barycenter']\n",
    "xd_sample_mmed = c_mardia_median(data_slice)\n",
    "xd_sample_bmed = marg_med(data_slice)\n",
    "xd_sample_hmean = rsc_mean(data_slice, sigma=mad_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f05842",
   "metadata": {},
   "source": [
    "Alternatively, we could take the median of the visibility amplitude and the Mardia median of the phase. While this is an improvement on doing the median on cartesian coordinates separately, it still does not wholly consider the complex data. The geometric median or the Tukey median would be preferable methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_ests = list(zip([xd_sample_gmean, xd_sample_gmed, xd_sample_tmed, xd_sample_mmed, \\\n",
    "                     xd_sample_bmed, xd_sample_hmean], \\\n",
    "               ['Geometric Mean', 'Geometric Median', 'Tukey Median', 'Mardia Median', \\\n",
    "                'Marginal Median', 'HERA Mean'], \\\n",
    "               ['co', 'ro', 'yo', 'ko', 'bo', 'go']))\n",
    "for me in med_ests:\n",
    "    print('{:17s}: {:4f}'.format(me[1], me[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fad85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.scatter(flt_nan(data_slice).real, flt_nan(data_slice).imag, alpha=0.5)\n",
    "for i, med_est in enumerate(med_ests):\n",
    "    ax.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "\n",
    "# zoomed in sub region of the original image\n",
    "axins = zoomed_inset_axes(ax, zoom=6, loc=4)\n",
    "# axins.scatter(flt_nan(data_slice).real, flt_nan(data_slice).imag, alpha=0.5, c='orange')\n",
    "for i, med_est in enumerate(med_ests):\n",
    "    axins.plot(med_est[0].real, med_est[0].imag, med_est[2])\n",
    "\n",
    "x1 = np.floor(np.min([i[0].real for i in med_ests[:-2]]))\n",
    "x2 = np.ceil(np.max([i[0].real for i in med_ests[:-2]]))\n",
    "y1 = np.floor(np.min([i[0].imag for i in med_ests[:-2]]))\n",
    "y2 = np.ceil(np.max([i[0].imag for i in med_ests[:-2]]))\n",
    "axins.set_xlim(x1, x2)\n",
    "axins.set_ylim(y1, y2)\n",
    "\n",
    "axins.tick_params(axis='x', direction='in', pad=-15)\n",
    "mark_inset(ax, axins, loc1=1, loc2=3, fc='none', ec='0.5')\n",
    "axins.patch.set_alpha(0.5)\n",
    "\n",
    "ax.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.03, 0.04), \\\n",
    "    xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'))\n",
    "ax.annotate('Chan: {}, LST: {:.3f}'.format(chans[idxs[0]], np.mean(xd_hr_lsts[[idxs[1], \\\n",
    "    idxs[1]+no_bins_agg-1]])), xy=(0.03, 0.95), xycoords='axes fraction', \\\n",
    "    bbox=dict(boxstyle='round', facecolor='white'))\n",
    "\n",
    "ax.set_xlabel(re_label)\n",
    "ax.set_ylabel(im_label)\n",
    "ax.set_title(textwrap.fill('Bivariate location estimators for redundant '\\\n",
    "    'visibilities aggregated across JDs', 60))\n",
    "\n",
    "ax.legend(loc=1, prop={'size': 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a3965",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=flt_nan(data_slice).real, y=flt_nan(data_slice).imag, \\\n",
    "                  kind='kde', height=8, cmap='Blues', fill=True, space=0)\n",
    "g.set_axis_labels(re_label, im_label, size=14)\n",
    "for i, med_est in enumerate(med_ests):\n",
    "    g.ax_joint.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "legend_properties = {'size': 10}\n",
    "g.ax_joint.legend(prop=legend_properties, loc='upper right')\n",
    "g.ax_joint.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.03, 0.04), \\\n",
    "    xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "    size=12)\n",
    "g.ax_joint.annotate('Chan: {}, LST: {:.3f}'.format(chans[idxs[0]], np.mean(xd_hr_lsts[[idxs[1], \\\n",
    "    idxs[1]+no_bins_agg-1]])), xy=(0.03, 0.95), xycoords='axes fraction', bbox=dict(boxstyle='round', \\\n",
    "    facecolor='white'), size=12)\n",
    "plt.tight_layout()\n",
    "# save_fig_dir = '/Users/matyasmolnar/Dropbox/PhD/Papers/memo_robstat/Figures/'\n",
    "# plt.savefig(os.path.join(save_fig_dir, 'density_big_diff_geo_hera.pdf'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bec739",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_red_res_fn = os.path.join(res_dir, os.path.basename(xd_vis_file).replace('.npz', '.lst_red_res.npz'))\n",
    "\n",
    "if not os.path.exists(lst_red_res_fn):\n",
    "\n",
    "    def freq_iter(freq):\n",
    "        xd_gmed_res_f = np.empty((1, new_no_tints), dtype=complex)\n",
    "        xd_tmed_res_f, xd_bmed_res_f, xd_hmean_res_f = [np.empty_like(xd_gmed_res_f) for \\\n",
    "                                                        _ in range(3)]\n",
    "\n",
    "        gmed_ft_init = None\n",
    "        for tint in range(new_no_tints):\n",
    "            if lstb_format:\n",
    "                xd_data_bft = xd_data_bls[:, freq, tint, :].flatten()\n",
    "            else:\n",
    "                xd_data_bft = xd_data_bls[:, freq, no_bins_agg*tint:no_bins_agg*tint+no_bins_agg, \\\n",
    "                                          :].flatten()\n",
    "\n",
    "            if np.isnan(xd_data_bft).all():\n",
    "                gmed_ft = tmed_ft = bmed_ft = hmean_ft = np.nan + 1j*np.nan\n",
    "            else:\n",
    "                gmed_ft = geometric_median(xd_data_bft, init_guess=gmed_ft_init, \\\n",
    "                                           options=dict(keep_res=True))\n",
    "                gmed_ft_init = gmed_ft\n",
    "                tmed_ft = tukey_median(xd_data_bft)['barycenter']\n",
    "                bmed_ft = marg_med(xd_data_bft)\n",
    "                hmean_ft = rsc_mean(xd_data_bft, sigma=mad_sigma)\n",
    "\n",
    "            xd_gmed_res_f[:, tint] = gmed_ft\n",
    "            xd_tmed_res_f[:, tint] = tmed_ft\n",
    "            xd_bmed_res_f[:, tint] = bmed_ft\n",
    "            xd_hmean_res_f[:, tint] = hmean_ft\n",
    "\n",
    "        return xd_gmed_res_f, xd_tmed_res_f, xd_bmed_res_f, xd_hmean_res_f\n",
    "\n",
    "    if mp:\n",
    "        m_pool = multiprocessing.Pool(multiprocessing.cpu_count())\n",
    "        pool_res = m_pool.map(freq_iter, range(no_chans))\n",
    "        m_pool.close()\n",
    "        m_pool.join()\n",
    "    else:\n",
    "        pool_res = list(map(freq_iter, range(no_chans)))\n",
    "\n",
    "    loc_res = np.concatenate(pool_res, axis=1)\n",
    "    xd_gmed_res = loc_res[0, ...]\n",
    "    xd_tmed_res = loc_res[1, ...]\n",
    "    xd_bmed_res = loc_res[2, ...]\n",
    "    xd_hmean_res = loc_res[3, ...]\n",
    "\n",
    "    np.savez(lst_red_res_fn, xd_gmed_res=xd_gmed_res, xd_tmed_res=xd_tmed_res, \\\n",
    "             xd_bmed_res=xd_bmed_res, xd_hmean_res=xd_hmean_res)\n",
    "\n",
    "else:\n",
    "    lst_red_res = np.load(lst_red_res_fn)\n",
    "    xd_gmed_res = lst_red_res['xd_gmed_res']\n",
    "    xd_tmed_res = lst_red_res['xd_tmed_res']\n",
    "    xd_bmed_res = lst_red_res['xd_bmed_res']\n",
    "    xd_hmean_res = lst_red_res['xd_hmean_res']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ad096",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = [xd_gmed_res, xd_tmed_res, xd_bmed_res, xd_hmean_res]\n",
    "\n",
    "tr_arrs = lambda x, np_fn: [getattr(np, np_fn)(i) for i in x]\n",
    "garrs = [tr_arrs(arrs, 'abs'), tr_arrs(arrs, 'angle'), tr_arrs(arrs, 'real'), tr_arrs(arrs, 'imag')]\n",
    "garrs = [[arr[i] for arr in garrs] for i in range(len(garrs[0]))]\n",
    "\n",
    "titles = ['Geometric Median', 'Tukey Median', 'Marginal Median', 'HERA Mean']\n",
    "ylabels = [r'$|V|$', r'$\\varphi$', re_label, im_label]\n",
    "ylabels = [ylab + '\\n\\nFrequency Channel' for ylab in ylabels]\n",
    "\n",
    "grid_heatmaps(garrs, titles=titles, figsize=(12, 10), xbase=tbase, ybase=fbase, clip_pctile=0, \\\n",
    "              xlabels='Time Bin', yticklabels=plt_chans, ylabels=ylabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lstb_format:\n",
    "\n",
    "    from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8), dpi=600)\n",
    "\n",
    "    grid = AxesGrid(fig, 111, nrows_ncols=(4, 4), axes_pad=0.1, share_all=True, cbar_location='right', \\\n",
    "                    cbar_mode='edge', cbar_size=0.1, cbar_pad=0.15, direction='row', aspect=False)\n",
    "\n",
    "    ylabels = [r'$|V|$', r'$\\varphi$', re_label, im_label]\n",
    "\n",
    "    for col, arr in enumerate(garrs):\n",
    "        for row, a in enumerate(arr):\n",
    "            idx = row*4 + col\n",
    "            ax = grid[idx]\n",
    "\n",
    "            if row == 1:\n",
    "                cmap = 'PiYG'\n",
    "                vmin = -np.pi\n",
    "                vmax = np.pi\n",
    "            else:\n",
    "                cmap = 'viridis'\n",
    "                vmin = None\n",
    "                vmax = None\n",
    "\n",
    "            im = ax.pcolormesh(np.arange(xd_data.shape[2]), plt_chans, a, rasterized=True, \n",
    "                vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "\n",
    "            if idx % 4 == 0:\n",
    "                grid.cbar_axes[row].colorbar(im, label=ylabels[row])\n",
    "\n",
    "            if row == 0:\n",
    "                ax.set_title(titles[col])\n",
    "\n",
    "            if row == 3:\n",
    "                ax.set_xlabel('Time Bin')\n",
    "                ax.set_xticks([0, 10, 20, 30, 40, 50])\n",
    "\n",
    "            if col == 0:\n",
    "                ax.set_ylabel('Frequency Channel')\n",
    "                ax.set_yticks([520, 560, 600, 640, 680])\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3a149",
   "metadata": {},
   "source": [
    "#### Smoothness of median results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e7f704",
   "metadata": {},
   "source": [
    "Calculate standard deviation of the distances between successive points in either frequency or time to get an idea of the smoothness of the location results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee968cf",
   "metadata": {},
   "source": [
    "##### Standard deviation of absolute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75dcef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in time\n",
    "t_smoothness = []\n",
    "for arr in arrs:\n",
    "    t_stds = np.empty(arr.shape[0])\n",
    "    for f in range(arr.shape[0]):\n",
    "        dists = np.abs(np.ediff1d(arr[f, :]))\n",
    "        t_stds[f] = np.nanstd(dists)\n",
    "    t_smoothness.append(np.nanmean(t_stds))\n",
    "print('Smoothness in time: \\n{}\\n{}\\n'.format(titles, t_smoothness))\n",
    "\n",
    "# in frequency\n",
    "f_smoothness = []\n",
    "for arr in arrs:\n",
    "    f_stds = np.empty(arr.shape[1])\n",
    "    for t in range(arr.shape[1]):\n",
    "        dists = np.abs(np.ediff1d(arr[:, t]))\n",
    "        f_stds[t] = np.nanstd(dists)\n",
    "    f_smoothness.append(np.nanmean(f_stds))\n",
    "print('Smoothness in frequency: \\n{}\\n{}'.format(titles, f_smoothness))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a7536",
   "metadata": {},
   "source": [
    "##### Standard deviation of complex differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb699276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in time\n",
    "t_smoothness = []\n",
    "for arr in arrs:\n",
    "    t_stds = np.empty(arr.shape[0])\n",
    "    for f in range(arr.shape[0]):\n",
    "        dists = np.ediff1d(arr[f, :])\n",
    "        t_stds[f] = np.nanstd(dists)\n",
    "    t_smoothness.append(np.nanmean(t_stds))\n",
    "print('Smoothness in time: \\n{}\\n{}\\n'.format(titles, t_smoothness))\n",
    "\n",
    "# in frequency\n",
    "f_smoothness = []\n",
    "for arr in arrs:\n",
    "    f_stds = np.empty(arr.shape[1])\n",
    "    for t in range(arr.shape[1]):\n",
    "        dists = np.ediff1d(arr[:, t])\n",
    "        f_stds[t] = np.nanstd(dists)\n",
    "    f_smoothness.append(np.nanmean(f_stds))\n",
    "print('Smoothness in frequency: \\n{}\\n{}'.format(titles, f_smoothness))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a81e4",
   "metadata": {},
   "source": [
    "#### Biggest difference in geometric median and HERA mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_slice = np.mean(flags, axis=(0, 3))\n",
    "ok_slice = ok_slice.reshape(no_chans, new_no_tints, -1).mean(axis=-1)\n",
    "ok_slice = ok_slice < 0.5 # only if less than 50% of flags are flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa00ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_argmax = np.nanargmax(np.abs(xd_gmed_res[ok_slice] - xd_hmean_res[ok_slice]))\n",
    "ok_slice_idxs = np.where(ok_slice)\n",
    "bd_idx = (ok_slice_idxs[0][ok_argmax], ok_slice_idxs[1][ok_argmax])\n",
    "\n",
    "print('Frequency/time slice {} shows strong deviation between the geometric median and the '\\\n",
    "      'HERA mean.'.format(bd_idx))\n",
    "\n",
    "if lstb_format:\n",
    "    bd_data = xd_data_bls[:, bd_idx[0], bd_idx[1], :].flatten()\n",
    "else:\n",
    "    bd_data = xd_data_bls[:, bd_idx[0], no_bins_agg*bd_idx[1]:no_bins_agg*bd_idx[1]+no_bins_agg, :].flatten()\n",
    "\n",
    "bd_med_ests = list(zip([xd_gmed_res[bd_idx], xd_hmean_res[bd_idx]], \\\n",
    "                       ['Geometric Median', 'HERA Mean'], \\\n",
    "                       ['ro', 'go']))\n",
    "\n",
    "g = sns.jointplot(x=flt_nan(bd_data).real, y=flt_nan(bd_data).imag, \\\n",
    "                  kind='kde', height=8, cmap='Blues', fill=True, space=0)\n",
    "g.set_axis_labels(re_label, im_label, size=14)\n",
    "for i, med_est in enumerate(bd_med_ests):\n",
    "    g.ax_joint.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "legend_properties = {'size': 10}\n",
    "g.ax_joint.legend(prop=legend_properties, loc='upper right')\n",
    "g.ax_joint.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "    xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "    size=12)\n",
    "g.ax_joint.annotate('Chan: {}, Tint: {}'.format(chans[bd_idx[0]], np.arange(new_no_tints)[bd_idx[1]]), \\\n",
    "    xy=(0.05, 0.95), xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "    size=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dbeaeb",
   "metadata": {},
   "source": [
    "#### More density plots\n",
    "\n",
    "To help visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick good (low flagging) data slices to plot as examples of redundant visibility distributions\n",
    "ok_idxs = np.where(ok_slice)\n",
    "index = np.random.choice(ok_idxs[0].shape[0], 2, replace=False)\n",
    "slice1 = ok_idxs[0][index[0]], ok_idxs[1][index[0]]\n",
    "slice2 = ok_idxs[0][index[1]], ok_idxs[1][index[1]]\n",
    "\n",
    "print('Random slices: {} & {}'.format(slice1, slice2))\n",
    "\n",
    "for rnd_slice in (slice1, slice2):\n",
    "\n",
    "    if lstb_format:\n",
    "        bd_data = xd_data_bls[:, rnd_slice[0], rnd_slice[1], :].flatten()\n",
    "    else:\n",
    "        bd_data = xd_data_bls[:, rnd_slice[0], no_bins_agg*rnd_slice[1]:no_bins_agg*rnd_slice[1]\\\n",
    "                              +no_bins_agg, :].flatten()\n",
    "\n",
    "    rnd_med_ests = list(zip([xd_gmed_res[rnd_slice], xd_hmean_res[rnd_slice]], \\\n",
    "                            ['Geometric Median', 'HERA Mean'], \\\n",
    "                            ['ro', 'go']))\n",
    "\n",
    "    g = sns.jointplot(x=flt_nan(bd_data).real, y=flt_nan(bd_data).imag, \\\n",
    "        kind='kde', height=8, cmap='Blues', fill=True, space=0)\n",
    "    g.set_axis_labels(re_label, im_label, size=14)\n",
    "    for i, med_est in enumerate(rnd_med_ests):\n",
    "        g.ax_joint.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "    legend_properties = {'size': 10}\n",
    "    g.ax_joint.legend(prop=legend_properties, loc='upper right')\n",
    "    g.ax_joint.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "        xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "        size=12)\n",
    "    g.ax_joint.annotate('Chan: {}, Tint: {}'.format(chans[rnd_slice[0]], \\\n",
    "        np.arange(new_no_tints)[rnd_slice[1]]), xy=(0.05, 0.95), xycoords='axes fraction', \\\n",
    "        bbox= dict(boxstyle='round', facecolor='white'), size=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plots across the H1C_IDR2 JDs for the same baseline group\n",
    "no_cols = 4\n",
    "no_rows = int(no_cols*np.ceil(JDs.size/no_cols) / no_cols)\n",
    "\n",
    "# select indices that have the lowest number of flags \n",
    "ft_flag_no = np.isnan(xd_data_bls).sum((0, 3)).reshape((no_chans, -1, no_bins_agg)).sum(axis=-1)\n",
    "slct_ft_idxs = np.array(np.unravel_index(np.argmin(ft_flag_no), ft_flag_no.shape))\n",
    "slct_ft_idxs[1] *= no_bins_agg\n",
    "slct_ft_idxs = tuple(slct_ft_idxs)\n",
    "\n",
    "pctc = 99\n",
    "pad = 5\n",
    "\n",
    "if lstb_format:\n",
    "    clip_data = xd_data_bls[:, slct_ft_idxs[0], slct_ft_idxs[1], :].flatten()\n",
    "else:\n",
    "    clip_data = xd_data_bls[:, slct_ft_idxs[0], slct_ft_idxs[1]:slct_ft_idxs[1]+no_bins_agg, :]\n",
    "\n",
    "re_lim = (np.floor(np.nanpercentile(clip_data.real, 100-pctc)) - pad, \\\n",
    "          np.nanpercentile(clip_data.real, pctc) + pad)\n",
    "im_lim = (np.floor(np.nanpercentile(clip_data.imag, 100-pctc)) - pad, \\\n",
    "          np.nanpercentile(clip_data.imag, pctc) + pad)\n",
    "\n",
    "gplots = []\n",
    "count = 0\n",
    "lcount = 0\n",
    "for row in range(no_rows):\n",
    "    for col in range(no_cols):\n",
    "        if (row*no_cols)+col <= JDs.size-1:\n",
    "            if lstb_format:\n",
    "                jd_data = xd_data_bls[count, slct_ft_idxs[0], slct_ft_idxs[1], :].flatten()\n",
    "            else:\n",
    "                jd_data = xd_data_bls[count, slct_ft_idxs[0], slct_ft_idxs[1]:slct_ft_idxs[1]\\\n",
    "                                      +no_bins_agg, :].flatten()\n",
    "            g = sns.jointplot(x=flt_nan(jd_data).real, y=flt_nan(jd_data).imag, \\\n",
    "                              kind='kde', height=8, cmap='Blues', fill=True, space=0, \\\n",
    "                              xlim=re_lim, ylim=im_lim)\n",
    "\n",
    "            if not np.isnan(jd_data).all():\n",
    "                jd_gmed = geometric_median(jd_data)\n",
    "                jd_hmean = rsc_mean(jd_data, sigma=mad_sigma)\n",
    "                g.ax_joint.plot(jd_gmed.real, jd_gmed.imag, 'ro', label='Geometric Median')\n",
    "                g.ax_joint.plot(jd_hmean.real, jd_hmean.imag, 'go', label='HERA Mean')\n",
    "                if lcount == 0:\n",
    "                    g.ax_joint.legend(prop={'size': 7}, loc='upper right')\n",
    "                lcount += 1\n",
    "\n",
    "            if count == 0:\n",
    "                g.ax_joint.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "                    xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "                    size=8)\n",
    "                g.ax_joint.annotate('Chan: {}, LST: {:.3f}'.format(chans[slct_ft_idxs[0]], \\\n",
    "                    np.mean(xd_hr_lsts[[slct_ft_idxs[1], slct_ft_idxs[1]+no_bins_agg-1]])), \\\n",
    "                    xy=(0.05, 0.95), xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "                    size=8)\n",
    "            g.ax_joint.annotate(str(JDs[count]), xy=(0.8, 0.05), \\\n",
    "                xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "                size=8)\n",
    "\n",
    "            g.set_axis_labels(re_label, im_label, size=8, labelpad=2)\n",
    "\n",
    "            gplots.append(g)\n",
    "            count += 1\n",
    "            plt.close() # suppress individual plots from showing in notebook\n",
    "\n",
    "fig = plt.figure(figsize=(16, 20))\n",
    "gs = gridspec.GridSpec(no_rows, no_cols)\n",
    "\n",
    "for i, gplot in enumerate(gplots):\n",
    "    _ = SeabornFig2Grid(gplot, fig, gs[i])\n",
    "\n",
    "gs.tight_layout(fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c141f87",
   "metadata": {},
   "source": [
    "### Test of normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1594632",
   "metadata": {},
   "source": [
    "#### Shapiro-Wilk test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d5ce6",
   "metadata": {},
   "source": [
    "We test the aggregated visibility data (over days, redundant baselines and consecutive time integrations) for normality using the Shapiro-Wilk test, to see if the data is Gaussian distributed for the $\\mathfrak{Re}$ and $\\mathfrak{Im}$ components separately, thus justifying the use of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21152f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_w_re = np.empty_like(xd_gmed_res, dtype=float)\n",
    "shapiro_w_im, shapiro_p_re, shapiro_p_im = [np.empty_like(shapiro_w_re) for _ in range(3)]\n",
    "for freq in range(no_chans):\n",
    "    for tint in range(new_no_tints):\n",
    "        if lstb_format:\n",
    "            xd_data_bft = flt_nan(xd_data_bls[:, freq, tint, :].flatten())\n",
    "        else:\n",
    "            xd_data_bft = flt_nan(xd_data_bls[:, freq, no_bins_agg*tint:no_bins_agg*tint+no_bins_agg, \\\n",
    "                                              :].flatten())\n",
    "        if np.isnan(xd_data_bft).all():\n",
    "            re_shapiro_stat = im_shapiro_stat = re_shapiro_pval = re_shapiro_pval = np.nan + 1j*np.nan\n",
    "        else:\n",
    "            re_shapiro = shapiro(xd_data_bft.real)\n",
    "            im_shapiro = shapiro(xd_data_bft.imag)\n",
    "            re_shapiro_stat = re_shapiro.statistic\n",
    "            im_shapiro_stat = im_shapiro.statistic\n",
    "            re_shapiro_pval = re_shapiro.pvalue\n",
    "            re_shapiro_pval = im_shapiro.pvalue\n",
    "\n",
    "        shapiro_w_re[freq, tint] = re_shapiro.statistic\n",
    "        shapiro_w_im[freq, tint] = im_shapiro.statistic\n",
    "        shapiro_p_re[freq, tint] = re_shapiro.pvalue\n",
    "        shapiro_p_im[freq, tint] = im_shapiro.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ef1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [[r'$W \\; \\mathrm{statistic} \\; - \\; \\mathfrak{Re}(V)$', \\\n",
    "          r'$p \\; \\mathrm{value} \\; - \\; \\mathfrak{Re}(V)$'], \\\n",
    "          [r'$W \\; \\mathrm{statistic} \\; - \\; \\mathfrak{Im}(V)$', \\\n",
    "          r'$p \\; \\mathrm{value} \\; - \\; \\mathfrak{Im}(V)$']]\n",
    "grid_heatmaps([[shapiro_w_re, shapiro_p_re], [shapiro_w_im, shapiro_p_im]], \\\n",
    "             titles=titles, figsize=(14, 7), xbase=tbase, ybase=fbase, share_cbar=True, clip_pctile=1, \\\n",
    "             xlabels='Time bin', yticklabels=plt_chans, ylabels='Frequency Channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0591bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example histograms for aggregated visibility data\n",
    "\n",
    "# picking frequency/time slice with worst shapiro statistic for Re visibilities\n",
    "re_shap_min = np.unravel_index(np.nanargmin(shapiro_p_re), shapiro_w_re.shape)\n",
    "print('Slice {} has Shapiro-Wilk test p value {:.5f} for the Re component.\\n'\\\n",
    "      .format(re_shap_min, shapiro_p_re[re_shap_min]))\n",
    "\n",
    "print('If the p value < the chosen alpha level (usually taken to be 0.05), then the null hypothesis '\\\n",
    "      'is rejected and there is evidence that the data tested are not normally distributed')\n",
    "\n",
    "hist_data = flt_nan(xd_data_bls[:, re_shap_min[0], re_shap_min[1], :])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(14, 7))\n",
    "\n",
    "sns.histplot(hist_data.real, ax=ax[0], binwidth=2.5, kde=True)\n",
    "sns.histplot(hist_data.imag, ax=ax[1], binwidth=2.5, kde=True)\n",
    "\n",
    "ax[0].set_xlabel(re_label)\n",
    "ax[1].set_xlabel(im_label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb612d7",
   "metadata": {},
   "source": [
    "#### Henze-Zirkler multivariate normality test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e4e02",
   "metadata": {},
   "source": [
    "We use the HZ test as this considers the entirety of the data. Note that many alternatives tests also exist and that a single statistic does not definitely conclude if the multivariate data is normality distributed or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0c109",
   "metadata": {},
   "source": [
    "##### LST + red aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAD-clipping about Re and Im separately, like HERA\n",
    "nan_flags = np.isnan(xd_data_bls)\n",
    "re_clip_f = mad_clip(xd_data_bls.real, sigma=mad_sigma, axis=(0, 3), flags=nan_flags, \\\n",
    "                     verbose=True)[1]\n",
    "im_clip_f = mad_clip(xd_data_bls.imag, sigma=mad_sigma, axis=(0, 3), flags=nan_flags, \\\n",
    "                     verbose=True)[1]\n",
    "\n",
    "xd_data_bls_c = xd_data_bls.copy()\n",
    "xd_data_bls_c[re_clip_f + im_clip_f] *= np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_nrm_res_fn = os.path.join(res_dir, os.path.basename(xd_vis_file).replace('.npz', '.mv_nrm_res.npz'))\n",
    "\n",
    "if not os.path.exists(mv_nrm_res_fn):\n",
    "\n",
    "    hz_r = np.empty_like(shapiro_w_re)\n",
    "    hz_p = np.empty_like(hz_r)\n",
    "    hz_n = np.empty_like(hz_r, dtype=bool)\n",
    "\n",
    "    hz_r_c = np.empty_like(hz_r)\n",
    "    hz_p_c = np.empty_like(hz_r)\n",
    "    hz_n_c = np.empty_like(hz_n)\n",
    "\n",
    "    bool_dict = {'NO': False, 'YES': True, np.nan: False}\n",
    "\n",
    "    for freq in range(no_chans):\n",
    "        for tint in range(new_no_tints):\n",
    "            if lstb_format:\n",
    "                xd_data_bft = flt_nan(xd_data_bls[:, freq, tint, :].flatten())\n",
    "                xd_data_bcft = flt_nan(xd_data_bls_c[:, freq, tint, :].flatten())\n",
    "            else:\n",
    "                xd_data_bft = flt_nan(xd_data_bls[:, freq, no_bins_agg*tint:no_bins_agg*tint+no_bins_agg, \\\n",
    "                                                  :].flatten())\n",
    "                xd_data_bcft = flt_nan(xd_data_bls_c[:, freq, no_bins_agg*tint:no_bins_agg*tint+no_bins_agg, \\\n",
    "                                                     :].flatten())\n",
    "\n",
    "            hz_res = mv_normality(xd_data_bft, method='hz')\n",
    "            hz_r[freq, tint] = hz_res['HZ']\n",
    "            hz_p[freq, tint] = hz_res['p value']\n",
    "            hz_n[freq, tint] = bool_dict[hz_res['MVN']]\n",
    "\n",
    "            hz_res_c = mv_normality(xd_data_bcft, method='hz')\n",
    "            hz_r_c[freq, tint] = hz_res_c['HZ']\n",
    "            hz_p_c[freq, tint] = hz_res_c['p value']\n",
    "            hz_n_c[freq, tint] = bool_dict[hz_res_c['MVN']]\n",
    "\n",
    "    np.savez(mv_nrm_res_fn, hz_r=hz_r, hz_p=hz_p, hz_n=hz_n, hz_r_c=hz_r_c, hz_p_c=hz_p_c, \\\n",
    "             hz_n_c=hz_n_c)\n",
    "\n",
    "else:\n",
    "    mv_nrm_res = np.load(mv_nrm_res_fn)\n",
    "    hz_r = mv_nrm_res['hz_r']\n",
    "    hz_p = mv_nrm_res['hz_p']\n",
    "    hz_n = mv_nrm_res['hz_n']\n",
    "    hz_r_c = mv_nrm_res['hz_r_c']\n",
    "    hz_p_c = mv_nrm_res['hz_p_c']\n",
    "    hz_n_c = mv_nrm_res['hz_n_c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fe331",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [r'$HZ$ Statistic', r'$p$-value', 'Normality']\n",
    "row_heatmaps([hz_r, hz_p, hz_n], titles=titles, figsize=(14, 7), share_cbar=False, \\\n",
    "             cbar_loc=None, clip_pctile=1, xlabels='Time Bin', ylabel='Frequency Channel', \\\n",
    "             yticklabels=plt_chans, xbase=tbase, ybase=fbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAD-clipped data\n",
    "titles = [r'$HZ$ Statistic', r'$p$-value', 'Normality']\n",
    "row_heatmaps([hz_r_c, hz_p_c, hz_n_c], titles=titles, figsize=(14, 7), share_cbar=False, \\\n",
    "             cbar_loc=None, clip_pctile=1, xlabels='Time Bin', ylabel='Frequency Channel', \\\n",
    "             yticklabels=plt_chans, xbase=tbase, ybase=fbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925afa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better grid plot of visibilities\n",
    "if lstb_format:\n",
    "\n",
    "    from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "    titles = [r'$HZ$ Statistic', r'$p$-value', 'Normality']\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 10), dpi=600)\n",
    "\n",
    "    grid = AxesGrid(fig, 111, nrows_ncols=(2, 3), axes_pad=0.15, share_all=True, cbar_location='bottom', \\\n",
    "                    cbar_mode='edge', cbar_size=0.15, cbar_pad=0.5, direction='row', aspect=False)\n",
    "\n",
    "    garrs = [[hz_r, hz_p, hz_n], [hz_r_c, hz_p_c, hz_n_c]]\n",
    "\n",
    "    for row, arr in enumerate(garrs):\n",
    "        for col, a in enumerate(arr):\n",
    "            idx = row*3 + col\n",
    "            ax = grid[idx]\n",
    "\n",
    "            if col == 0:\n",
    "                vmin = 0\n",
    "                vmax = 8\n",
    "                extend = 'max'\n",
    "                cmap = 'RdPu'\n",
    "            elif col == 1:\n",
    "                vmin = 0\n",
    "                vmax = 0.6\n",
    "                extend = 'max'\n",
    "                cmap = 'RdPu'\n",
    "            elif col == 2:\n",
    "                vmin = 0\n",
    "                vmax = 1\n",
    "                extend = None\n",
    "                bool_colors = ((1.0, 0.0, 0.0), (0.0, 0.0, 1.0))\n",
    "                cmap = LinearSegmentedColormap.from_list('Custom', bool_colors, len(bool_colors))\n",
    "\n",
    "                if row == 0:\n",
    "                    ax.annotate('pre-MAD', xy=(0.65, 0.94), xycoords='axes fraction', \\\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=1))\n",
    "                elif row == 1:\n",
    "                    ax.annotate('post-MAD', xy=(0.62, 0.94), xycoords='axes fraction', \\\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=1))\n",
    "\n",
    "            im = ax.pcolormesh(np.arange(xd_data.shape[2]), plt_chans, a, rasterized=True, \n",
    "                vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "\n",
    "            if row == 0:\n",
    "                ax.set_title(titles[col])\n",
    "\n",
    "            if row == 1:\n",
    "                ax.set_xlabel('Time Bin')\n",
    "                ax.set_xticks([0, 10, 20, 30, 40, 50])\n",
    "                cbar = grid.cbar_axes[col].colorbar(im, extend=extend)\n",
    "                if col == 2:\n",
    "                    cbar.set_ticks([0.25,0.75])\n",
    "                    cbar.set_ticklabels(['False', 'True'])\n",
    "\n",
    "            if col == 0:\n",
    "                ax.set_ylabel('Frequency Channel')\n",
    "                ax.set_yticks([520, 560, 600, 640, 680])            \n",
    "\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fa77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking frequency/time slice with worst HZ statistic\n",
    "hz_p_min = np.unravel_index(np.nanargmin(hz_p), hz_p.shape)\n",
    "print('Slice {}: Chan {} & LST {:.4f} has HZ test p value {:.5f}.\\n'\\\n",
    "      .format(hz_p_min, chans[hz_p_min[0]], avg_hr_lsts[hz_p_min[1]], hz_p[hz_p_min]))\n",
    "\n",
    "print('If the p value < the chosen alpha level (usually taken to be 0.05), then the null hypothesis '\\\n",
    "      'is rejected and there is evidence that the data tested are not normally distributed')\n",
    "\n",
    "if lstb_format:\n",
    "    hz_data = flt_nan(xd_data_bls[:, hz_p_min[0], hz_p_min[1], :].flatten())\n",
    "else:\n",
    "    hz_data = flt_nan(xd_data_bls[:, hz_p_min[0], no_bins_agg*hz_p_min[1]:no_bins_agg*hz_p_min[1]\\\n",
    "                      +no_bins_agg, :].flatten())\n",
    "\n",
    "bhz_med_ests = list(zip([xd_gmed_res[hz_p_min], xd_hmean_res[hz_p_min]], \\\n",
    "                        ['Geometric Median', 'HERA Mean'], \\\n",
    "                        ['ro', 'go']))\n",
    "\n",
    "g = sns.jointplot(x=hz_data.real, y=hz_data.imag, \\\n",
    "                  kind='kde', height=8, cmap='Blues', fill=True, space=0)\n",
    "for i, med_est in enumerate(bhz_med_ests):\n",
    "    g.ax_joint.plot(med_est[0].real, med_est[0].imag, med_est[2], label=med_est[1])\n",
    "g.set_axis_labels(re_label, im_label, size=14)\n",
    "legend_properties = {'size': 10}\n",
    "g.ax_joint.legend(prop=legend_properties, loc='upper right')\n",
    "g.ax_joint.annotate(tuple(slct_red_bl) + (str(xd_pol),), xy=(0.05, 0.05), \\\n",
    "    xycoords='axes fraction', bbox= dict(boxstyle='round', facecolor='white'), \\\n",
    "    size=12)\n",
    "g.ax_joint.annotate('Chan: {}, LST: {:.3f}'.format(chans[hz_p_min[0]], avg_hr_lsts[hz_p_min[1]]), \\\n",
    "    xy=(0.05, 0.95), xycoords='axes fraction', bbox=dict(boxstyle='round', facecolor='white'), size=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c357cab",
   "metadata": {},
   "source": [
    "##### LST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb9c1e",
   "metadata": {},
   "source": [
    "We look at the multivariate normality of data aggregated over days only - we then average this statistic over baselines to get a more complete picture of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_nrm_lst_res_fn = os.path.join(res_dir, os.path.basename(xd_vis_file).\n",
    "                                 replace('.npz', '.mv_nrm_lst_res.npz'))\n",
    "\n",
    "if not os.path.exists(mv_nrm_lst_res_fn):\n",
    "\n",
    "    hz_lst_r = np.empty((no_chans, new_no_tints, no_bls), dtype=float)\n",
    "    hz_lst_p = np.empty_like(hz_lst_r)\n",
    "    hz_lst_n = np.empty_like(hz_lst_r, dtype=bool)\n",
    "\n",
    "    bool_dict = {'NO': False, 'YES': True, np.nan: False}\n",
    "\n",
    "    for bl in range(no_bls):\n",
    "        for freq in range(no_chans):\n",
    "            for tint in range(new_no_tints):\n",
    "                if lstb_format:\n",
    "                    xd_data_bft = flt_nan(xd_data_bls[:, freq, tint, bl].flatten())\n",
    "                else:\n",
    "                    xd_data_bft = flt_nan(xd_data_bls[:, freq, no_bins_agg*tint:no_bins_agg*tint+no_bins_agg, \\\n",
    "                                                      bl].flatten())\n",
    "\n",
    "                hz_res = mv_normality(xd_data_bft, method='hz')\n",
    "                hz_lst_r[freq, tint, bl] = hz_res['HZ']\n",
    "                hz_lst_p[freq, tint, bl] = hz_res['p value']\n",
    "                hz_lst_n[freq, tint, bl] = bool_dict[hz_res['MVN']]\n",
    "\n",
    "    np.savez(mv_nrm_lst_res_fn, hz_lst_r=hz_lst_r, hz_lst_p=hz_lst_p, hz_lst_n=hz_lst_n)\n",
    "\n",
    "else:\n",
    "    mv_nrm_lst_res = np.load(mv_nrm_lst_res_fn)\n",
    "    hz_lst_r = mv_nrm_lst_res['hz_lst_r']\n",
    "    hz_lst_p = mv_nrm_lst_res['hz_lst_p']\n",
    "    hz_lst_n = mv_nrm_lst_res['hz_lst_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8746ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [r'$HZ \\; \\mathrm{statistic}$', r'$p \\; \\mathrm{value}$', 'Normality']\n",
    "hz_lst_res = [np.nanmean(hz_lst_r, axis=-1), np.nanmean(hz_lst_p, axis=-1), \\\n",
    "              np.nanmean(hz_lst_n.astype(float), axis=-1)]  # mean across baselines\n",
    "\n",
    "row_heatmaps(hz_lst_res, titles=titles, figsize=(14, 7), share_cbar=False, \\\n",
    "             cbar_loc=None, clip_pctile=1, xlabels='Time Bin', ylabel='Frequency Channel', \\\n",
    "             yticklabels=plt_chans, xbase=tbase, ybase=fbase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da797064",
   "metadata": {},
   "source": [
    "##### Redundant baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ff4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_nrm_red_res_fn = os.path.join(res_dir, os.path.basename(xd_vis_file).\n",
    "                                 replace('.npz', '.mv_nrm_red_res.npz'))\n",
    "\n",
    "if not os.path.exists(mv_nrm_red_res_fn):\n",
    "\n",
    "    hz_red_r = np.empty((no_days, no_chans, new_no_tints), dtype=float)\n",
    "    hz_red_p = np.empty_like(hz_red_r)\n",
    "    hz_red_n = np.empty_like(hz_red_r, dtype=bool)\n",
    "\n",
    "    bool_dict = {'NO': False, 'YES': True, np.nan: False}\n",
    "\n",
    "    for day in range(no_days):\n",
    "        for freq in range(no_chans):\n",
    "            for tint in range(new_no_tints):\n",
    "                if lstb_format:\n",
    "                    xd_data_bft = flt_nan(xd_data_bls[day, freq, tint, :].flatten())\n",
    "                else:\n",
    "                    xd_data_bft = flt_nan(xd_data_bls[day, freq, no_bins_agg*tint:no_bins_agg*tint+no_bins_agg, \\\n",
    "                                                      :].flatten())\n",
    "\n",
    "                hz_res = mv_normality(xd_data_bft, method='hz')\n",
    "                hz_red_r[day, freq, tint] = hz_res['HZ']\n",
    "                hz_red_p[day, freq, tint] = hz_res['p value']\n",
    "                hz_red_n[day, freq, tint] = bool_dict[hz_res['MVN']]\n",
    "\n",
    "    np.savez(mv_nrm_red_res_fn, hz_red_r=hz_red_r, hz_red_p=hz_red_p, hz_red_n=hz_red_n)\n",
    "\n",
    "else:\n",
    "    mv_nrm_red_res = np.load(mv_nrm_red_res_fn)\n",
    "    hz_red_r = mv_nrm_red_res['hz_red_r']\n",
    "    hz_red_p = mv_nrm_red_res['hz_red_p']\n",
    "    hz_red_n = mv_nrm_red_res['hz_red_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b39cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [r'$HZ \\; \\mathrm{statistic}$', r'$p \\; \\mathrm{value}$', 'Normality']\n",
    "hz_red_res = [np.nanmean(hz_red_r, axis=0), np.nanmean(hz_red_p, axis=0), \\\n",
    "              np.nanmean(hz_red_n.astype(float), axis=0)] # mean across days\n",
    "\n",
    "row_heatmaps(hz_red_res, titles=titles, figsize=(14, 7), share_cbar=False, \\\n",
    "             cbar_loc=None, clip_pctile=1, xlabels='Time Bin', ylabel='Frequency Channel', \\\n",
    "             yticklabels=plt_chans, xbase=tbase, ybase=fbase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca147b8",
   "metadata": {},
   "source": [
    "### Multivariate outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7984ca",
   "metadata": {},
   "source": [
    "We use the robust Mahalanobis distance to detect outliers in the complex HERA data, as opposed to performing MAD-clipping on the $\\mathfrak{Re}$ and$\\mathfrak{Im}$ components separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12c8e9",
   "metadata": {},
   "source": [
    "#### Slice with worst HZ statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d242367",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvo_res = mv_outlier(hz_data)\n",
    "mvo_res.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAD-clipping about Re and Im separately\n",
    "re_clip_f_mvo = mad_clip(hz_data.real, sigma=mad_sigma, verbose=True)[1]\n",
    "im_clip_f_mvo = mad_clip(hz_data.imag, sigma=mad_sigma, verbose=True)[1]\n",
    "\n",
    "mvo_res['MAD-clip'] = re_clip_f_mvo + im_clip_f_mvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(14, 6), sharey=True)\n",
    "sns.scatterplot(x=hz_data.real, y=hz_data.imag, hue=mvo_res['RS Mahalanobis Distance'], \\\n",
    "                ax=axes[0])\n",
    "sns.scatterplot(x=hz_data.real, y=hz_data.imag, hue=mvo_res['Outlier'], ax=axes[1])\n",
    "sns.scatterplot(x=hz_data.real, y=hz_data.imag, hue=mvo_res['MAD-clip'], ax=axes[2])\n",
    "axes[0].set_ylabel(r'$\\mathfrak{Im} \\; (V)$')\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_xlabel(re_label)\n",
    "    ax.plot(bhz_med_ests[0][0].real, bhz_med_ests[0][0].imag, bhz_med_ests[0][2])\n",
    "    legend_title = mvo_res.columns.values[1:][i]\n",
    "    ax.legend(loc='lower right', title=legend_title)\n",
    "axes[-1].annotate('Geometric Median', xy =(0.65, 0.95), xycoords='axes fraction', color='r', \\\n",
    "                  bbox= dict(boxstyle='round', facecolor='white'))\n",
    "axes[0].annotate('Chan: {}, LST: {:.3f}'.format(chans[hz_p_min[0]], avg_hr_lsts[hz_p_min[1]]), \\\n",
    "    xy=(0.05, 0.95), xycoords='axes fraction', bbox=dict(boxstyle='round', facecolor='white'), size=12, \\\n",
    "    alpha=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 97.5% quantile of the chi-square distribution is classically taken for outlier threshold\n",
    "# let's look at a stricter threshold:\n",
    "chi2_quantile = 0.99\n",
    "strct_outliers = np.where(mvo_res['RS Mahalanobis Distance'].values \\\n",
    "                          > chi2.ppf(chi2_quantile, 2))[0]\n",
    "print('Outliers when taking the chi-square quantile to be {}% are:'.format(chi2_quantile*100))\n",
    "# print(*np.around(hz_data[strct_outliers], decimals=5).tolist(), sep='\\n')\n",
    "for i, s_outlier in enumerate(np.around(hz_data[strct_outliers], decimals=5)):\n",
    "    print('{:19.5f}'.format(s_outlier), end = '  ' if (i+1) % 5 else '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711d735",
   "metadata": {},
   "source": [
    "#### Sifting through the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3562661",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dp = xd_data_bls.shape[0]*no_bls*no_bins_agg\n",
    "\n",
    "mah_out_res_fn = os.path.join(res_dir, os.path.basename(xd_vis_file).replace('.npz', '.mah_out_res.npz'))\n",
    "\n",
    "if not os.path.exists(mah_out_res_fn):\n",
    "\n",
    "    mah_outliers = np.empty((no_chans, new_no_tints, no_dp), dtype=bool)\n",
    "\n",
    "    uf_xd_data = sample_xd_data['data'][..., slct_bl_idxs]\n",
    "\n",
    "    for freq in range(no_chans):\n",
    "        for tint in range(new_no_tints):\n",
    "            if lstb_format:\n",
    "                xd_data_bft = uf_xd_data[:, chans[freq], tint, :].flatten()\n",
    "            else:\n",
    "                xd_data_bft = uf_xd_data[:, freq, no_bins_agg*tint:no_bins_agg*tint+no_bins_agg, \\\n",
    "                                         :].flatten()\n",
    "\n",
    "            if np.isnan(xd_data_bft).all():\n",
    "                out_ft = np.empty(no_dp)*np.nan\n",
    "            else:\n",
    "                out_ft = mv_outlier(xd_data_bft)['Outlier']\n",
    "\n",
    "            mah_outliers[freq, tint, :] = out_ft\n",
    "\n",
    "    np.savez(mah_out_res_fn, mah_outliers=mah_outliers)\n",
    "\n",
    "else:\n",
    "    mah_outliers = np.load(mah_out_res_fn)['mah_outliers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_outliers = mah_outliers.sum(axis=-1)/no_dp*100\n",
    "row_heatmaps(no_outliers, clip_pctile=2, xlabels='Time Bin', ylabel='Frequency Channel', \\\n",
    "             titles=['Percentage of of outliers found with the robust Mahalanobis distance '\\\n",
    "             'technique'], yticklabels=plt_chans, xbase=tbase, ybase=fbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed536325",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_flags_xdbl = flags.sum(axis=(0, -1))\n",
    "cal_flags = cal_flags_xdbl.reshape((no_chans, new_no_tints, -1)).sum(axis=-1)\n",
    "\n",
    "mad_flags_xdbl = (re_clip_f + im_clip_f).sum(axis=(0, -1))\n",
    "mad_flags = mad_flags_xdbl.reshape((no_chans, new_no_tints, -1)).sum(axis=-1)\n",
    "\n",
    "comb_flags_xdbl = (flags + re_clip_f + im_clip_f).sum(axis=(0, -1))\n",
    "comb_flags = comb_flags_xdbl.reshape((no_chans, new_no_tints, -1)).sum(axis=-1)\n",
    "\n",
    "cal_f_pct = cal_flags / no_dp*100\n",
    "mad_f_pct = mad_flags / no_dp*100\n",
    "comb_f_pct = comb_flags / no_dp*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff5069",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=['Percentage of of flagged data from calibration', \\\n",
    "        'Percentage of of flagged data from MAD-clipping', \\\n",
    "        'Percentage of of flagged data from calibration + MAD-clipping']\n",
    "titles = [textwrap.fill(t, 40) for t in titles]\n",
    "\n",
    "row_heatmaps([cal_f_pct, mad_f_pct, comb_f_pct], clip_pctile=2, figsize=(14, 6), \\\n",
    "             titles=titles, xlabels='Time Bin', ylabel='Frequency Channel', yticklabels=plt_chans, \\\n",
    "             xbase=tbase, ybase=fbase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884cbdd",
   "metadata": {},
   "source": [
    "### Statistical properties of location estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6469136",
   "metadata": {},
   "source": [
    "#### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ad547",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 8), sharey='row', sharex='col')\n",
    "\n",
    "axes[0][0].plot(np.abs(xd_gmed_res), alpha=0.7)\n",
    "axes[0][1].plot(np.abs(xd_hmean_res), alpha=0.7)\n",
    "axes[1][0].plot(np.angle(xd_gmed_res), alpha=0.7)\n",
    "axes[1][1].plot(np.angle(xd_hmean_res), alpha=0.7)\n",
    "\n",
    "axes[0][0].set_ylabel(r'$|V|$')\n",
    "axes[1][0].set_ylabel(r'$\\varphi$')\n",
    "\n",
    "axes[1][0].set_xlabel('Frequency Channel')\n",
    "axes[1][1].set_xlabel('Frequency Channel')\n",
    "\n",
    "axes[0][0].set_title('Geometric Median')\n",
    "axes[0][1].set_title('HERA Mean')\n",
    "\n",
    "for axr in axes:\n",
    "    for axc in axr:\n",
    "        axc.set_xticks(np.arange(plt_chans.size)[::fbase])\n",
    "        axc.set_xticklabels(plt_chans[::fbase])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a81ba0",
   "metadata": {},
   "source": [
    "##### Fill in gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690611e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid interpolation to replace nan values\n",
    "gmed_interp2 = nan_interp2d(xd_gmed_res)\n",
    "hmean_interp2 = nan_interp2d(xd_hmean_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01636d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_chans = extrem_nans(np.isnan(xd_gmed_res).all(axis=1))\n",
    "flt_chans = chans.copy()\n",
    "if nan_chans.size != 0:\n",
    "    flt_chans = np.delete(flt_chans, nan_chans, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464675e2",
   "metadata": {},
   "source": [
    "### Nonparametric kernel regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96111891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D kernel regression for the visibility amplitudes\n",
    "\n",
    "kde_abs_gmed = np.empty_like(gmed_interp2, dtype=float)\n",
    "kde_abs_hmean = np.empty_like(kde_abs_gmed)\n",
    "\n",
    "for btint in range(gmed_interp2.shape[1]):\n",
    "    kde_gmed = KernelReg(endog=np.abs(gmed_interp2[:, btint]), exog=flt_chans, \\\n",
    "                         reg_type='ll', var_type='c', bw=[3])\n",
    "    kde_abs_gmed[:, btint] = kde_gmed.fit(flt_chans)[0]\n",
    "\n",
    "    kde_hmean = KernelReg(endog=np.abs(hmean_interp2[:, btint]), exog=flt_chans, \\\n",
    "                          reg_type='ll', var_type='c', bw=[3])\n",
    "    kde_abs_hmean[:, btint] = kde_hmean.fit(flt_chans)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6), sharex='col', sharey='row')\n",
    "\n",
    "axes[0][0].plot(flt_chans, kde_abs_gmed, alpha=0.7)\n",
    "axes[0][1].plot(flt_chans, kde_abs_hmean, alpha=0.7)\n",
    "\n",
    "gmed_residual = np.abs(gmed_interp2) - kde_abs_gmed\n",
    "axes[1][0].plot(flt_chans, gmed_residual, alpha=0.3)\n",
    "axes[1][0].plot(flt_chans, gmed_residual.mean(axis=1))\n",
    "\n",
    "hmean_residual = np.abs(hmean_interp2) - kde_abs_hmean\n",
    "axes[1][1].plot(flt_chans, hmean_residual, alpha=0.3)\n",
    "axes[1][1].plot(flt_chans, hmean_residual.mean(axis=1))\n",
    "\n",
    "axes[1][0].set_xlabel('Frequency Channel')\n",
    "axes[1][1].set_xlabel('Frequency Channel')\n",
    "axes[0][0].set_ylabel(r'$|V|$')\n",
    "axes[1][0].set_ylabel('Residual')\n",
    "\n",
    "axes[0][0].set_title('Geometric Median KDE')\n",
    "axes[0][1].set_title('HERA Mean KDE')\n",
    "\n",
    "for axr in axes:\n",
    "    for axc in axr:\n",
    "        axc.set_xticks(flt_chans[::fbase])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18033fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot of complex visibilities with geometric median taken across days and baselines\n",
    "slct_btint = 0\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = plt.axes(projection='3d')\n",
    "for i, chan in enumerate(range(gmed_interp2.shape[0])):\n",
    "    ax.scatter(gmed_interp2[chan, :].real, gmed_interp2[chan, :].imag, \\\n",
    "               np.repeat(flt_chans[i], new_no_tints), c=np.arange(new_no_tints), \\\n",
    "               cmap='Greens', alpha=0.5, s=7)\n",
    "ax.set_xlabel(re_label)\n",
    "ax.set_ylabel(im_label)\n",
    "ax.set_zlabel('Frequency Channel')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49bfbf6",
   "metadata": {},
   "source": [
    "#### Allan deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fcc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_resolution = np.median(np.ediff1d(freqs))\n",
    "\n",
    "try:\n",
    "    import allantools\n",
    "    t_resolution = np.median(hd.integration_time)\n",
    "\n",
    "    rate_exp = np.log10(f_resolution)\n",
    "    tau_min = np.ceil(np.abs(rate_exp))*np.sign(rate_exp)\n",
    "\n",
    "    taus = np.logspace(tau_min, tau_min+np.ceil(np.log10(gmed_interp2.shape[0])), 1000)\n",
    "\n",
    "    gmed_ads = np.empty((int(np.ceil(gmed_interp2.shape[0]/2)-1), gmed_interp2.shape[1]))\n",
    "    hmean_ads = np.empty_like(gmed_ads)\n",
    "\n",
    "    for btint in range(gmed_interp2.shape[1]):\n",
    "        # do OADEV on residuals rather than on signal with structure\n",
    "        gmed_taus2, gmed_ad, gmed_ade, gmed_ns = allantools.oadev(gmed_residual[:, btint], \\\n",
    "            rate=1/f_resolution, data_type='freq', taus=taus)\n",
    "\n",
    "        hmean_taus2, hmean_ad, hmean_ade, hmean_ns = allantools.oadev(hmean_residual[:, btint], \\\n",
    "            rate=1/f_resolution, data_type='freq', taus=taus)\n",
    "\n",
    "        gmed_ads[:, btint] = gmed_ad\n",
    "        hmean_ads[:, btint] = hmean_ad\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(10, 6), sharey=True)\n",
    "\n",
    "    ax[0].loglog(gmed_taus2, gmed_ads, alpha=0.5)\n",
    "    ax[1].loglog(hmean_taus2, hmean_ads, alpha=0.5)\n",
    "\n",
    "    ax[0].set_title('Geometric Median')\n",
    "    ax[1].set_title('HERA Mean')\n",
    "\n",
    "    ax[0].set_ylabel('Overlapping Allan deviation')\n",
    "    ax[0].set_xlabel(r'$\\tau$')\n",
    "    ax[1].set_xlabel(r'$\\tau$')\n",
    "\n",
    "    plt.suptitle('Allan deviation')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    # get AllanTools package here https://github.com/aewallin/allantools\n",
    "    # or do pip install allantools\n",
    "    print('AllanTools package not installed - skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a2c8c",
   "metadata": {},
   "source": [
    "#### Power spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36980c6",
   "metadata": {},
   "source": [
    "##### Single time integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmed_delay, gmed_pspec = signal.periodogram(gmed_interp2[:, 0], fs=1/f_resolution, \\\n",
    "    window='hann', scaling='spectrum', nfft=None, detrend=False, \\\n",
    "    return_onesided=False)\n",
    "\n",
    "delay_sort = np.argsort(gmed_delay)\n",
    "gmed_delay = gmed_delay[delay_sort]\n",
    "gmed_pspec = gmed_pspec[delay_sort]\n",
    "\n",
    "hmean_delay, hmean_pspec = signal.periodogram(hmean_interp2[:, 0], fs=1./f_resolution, \\\n",
    "    window='hann', scaling='spectrum', nfft=None, detrend=False, \\\n",
    "    return_onesided=False)\n",
    "\n",
    "delay_sort = np.argsort(hmean_delay)\n",
    "hmean_delay = hmean_delay[delay_sort]\n",
    "hmean_pspec = hmean_pspec[delay_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef022c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(gmed_delay, gmed_pspec, label='Geometric Median', alpha=0.8)\n",
    "ax.plot(hmean_delay, hmean_pspec, label='HERA Mean', alpha=0.8)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('Power spectrum')\n",
    "ax.set_xlabel('Delay')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6687d76",
   "metadata": {},
   "source": [
    "##### All time integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a088eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmed_delay, gmed_pspec = signal.periodogram(gmed_interp2, fs=1/f_resolution, \\\n",
    "    window='hann', scaling='spectrum', nfft=None, detrend=False, \\\n",
    "    return_onesided=False, axis=0)\n",
    "\n",
    "delay_sort = np.argsort(gmed_delay)\n",
    "gmed_delay = gmed_delay[delay_sort]\n",
    "gmed_pspec = gmed_pspec[delay_sort, :]\n",
    "\n",
    "hmean_delay, hmean_pspec = signal.periodogram(hmean_interp2, fs=1/f_resolution, \\\n",
    "    window='hann', scaling='spectrum', nfft=None, detrend=False, \\\n",
    "    return_onesided=False, axis=0)\n",
    "\n",
    "delay_sort = np.argsort(hmean_delay)\n",
    "hmean_delay = hmean_delay[delay_sort]\n",
    "hmean_pspec = hmean_pspec[delay_sort, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a7ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(7, 5), sharey=True)\n",
    "\n",
    "axes[0].plot(gmed_delay*1e6, gmed_pspec, alpha=0.3)\n",
    "axes[0].plot(gmed_delay*1e6, gmed_pspec.mean(axis=1), alpha=1, color='orange')\n",
    "axes[0].set_ylabel(r'Power Spectrum [Jy$^2$ Hz$^2$]')\n",
    "\n",
    "axes[1].plot(hmean_delay*1e6, hmean_pspec, alpha=0.3)\n",
    "axes[1].plot(hmean_delay*1e6, hmean_pspec.mean(axis=1), alpha=1, color='purple')\n",
    "\n",
    "axes[2].plot(gmed_delay*1e6, gmed_pspec.mean(axis=1), alpha=0.7, color='orange', label='Geometric Median')\n",
    "axes[2].plot(hmean_delay*1e6, hmean_pspec.mean(axis=1), alpha=0.7, color='purple', label='HERA Mean')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Delay [$\\mu$s]')\n",
    "    ax.set_xticks([-5, -2.5, 0, 2.5, 5])\n",
    "\n",
    "axes[0].set_title('Geometric Median')\n",
    "axes[1].set_title('HERA Mean')\n",
    "axes[2].set_title('Comparison')\n",
    "axes[2].legend(loc='lower center')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638cc6b",
   "metadata": {},
   "source": [
    "##### Cross-power spectrum between neighbouring time bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cbe6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmed_interp2_1 = gmed_interp2[:, ::2]\n",
    "gmed_interp2_2 = gmed_interp2[:, 1::2]\n",
    "\n",
    "hmean_interp2_1 = hmean_interp2[:, ::2]\n",
    "hmean_interp2_2 = hmean_interp2[:, 1::2]\n",
    "\n",
    "gmed_delay, gmed_pspec = signal.csd(gmed_interp2_1, gmed_interp2_2, fs=1/f_resolution, \\\n",
    "    window='hann', scaling='spectrum', nfft=None, detrend=False, \\\n",
    "    nperseg=gmed_interp2_1.shape[0], return_onesided=False, axis=0)\n",
    "\n",
    "delay_sort = np.argsort(gmed_delay)\n",
    "gmed_delay = gmed_delay[delay_sort]\n",
    "gmed_pspec = gmed_pspec[delay_sort, :]\n",
    "\n",
    "hmean_delay, hmean_pspec = signal.csd(hmean_interp2_1, hmean_interp2_2, fs=1/f_resolution, \\\n",
    "    window='hann', scaling='spectrum', nfft=None, detrend=False, \\\n",
    "    nperseg=hmean_interp2_1.shape[0], return_onesided=False, axis=0)\n",
    "\n",
    "delay_sort = np.argsort(hmean_delay)\n",
    "hmean_delay = hmean_delay[delay_sort]\n",
    "hmean_pspec = hmean_pspec[delay_sort, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ce392",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(7, 5), sharey=True)\n",
    "\n",
    "axes[0].plot(gmed_delay*1e6, np.abs(gmed_pspec), alpha=0.3)\n",
    "axes[0].plot(gmed_delay*1e6, np.abs(gmed_pspec.mean(axis=1)), alpha=1, color='orange')\n",
    "axes[0].set_ylabel(r'Cross Power Spectrum [Jy$^2$ Hz$^2$]')\n",
    "\n",
    "axes[1].plot(hmean_delay*1e6, np.abs(hmean_pspec), alpha=0.3)\n",
    "axes[1].plot(hmean_delay*1e6, np.abs(hmean_pspec.mean(axis=1)), alpha=1, color='purple')\n",
    "\n",
    "axes[2].plot(gmed_delay*1e6, np.abs(gmed_pspec.mean(axis=1)), alpha=0.8, color='orange', label='Geometric Median')\n",
    "axes[2].plot(hmean_delay*1e6, np.abs(hmean_pspec.mean(axis=1)), alpha=0.8, color='purple', label='HERA Mean')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Delay [$\\mu$s]')\n",
    "#     ax.set_xticks([-5, -2.5, 0, 2.5, 5])\n",
    "#     ax.set_ylim(5e-7, 1e4)\n",
    "\n",
    "axes[0].set_title('Geometric Median')\n",
    "axes[1].set_title('HERA Mean')\n",
    "axes[2].set_title('Comparison')\n",
    "axes[2].legend(loc='lower center')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b52ad",
   "metadata": {},
   "source": [
    "### Only average visibilities across days\n",
    "\n",
    "And further average across baselines post power spectrum computation by computing cross-power spectrum across all baseline permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_full_res_fn = os.path.join(res_dir, os.path.basename(xd_vis_file).replace('.npz', '.lst_full_res.npz'))\n",
    "\n",
    "if not os.path.exists(lst_full_res_fn):\n",
    "\n",
    "    bls_to_do = np.arange(no_bls)[slct_no_bls:]\n",
    "\n",
    "    def freq_iter(freq):\n",
    "        xd_gmed_res_bl_f = np.empty((1, new_no_tints, bls_to_do.size), dtype=complex)\n",
    "        xd_hmean_res_bl_f = np.empty_like(xd_gmed_res_bl_f)\n",
    "\n",
    "        gmed_ft_init = None\n",
    "        for b, bl in enumerate(bls_to_do):\n",
    "            for tint in range(new_no_tints):\n",
    "                if lstb_format:\n",
    "                    xd_data_bft = xd_data_bls[:, freq, tint, bl].flatten()\n",
    "                else:\n",
    "                    xd_data_bft = xd_data_bls[:, freq, no_bins_agg*tint:no_bins_agg*tint+no_bins_agg, \\\n",
    "                                              bl].flatten()\n",
    "\n",
    "                if np.isnan(xd_data_bft).all():\n",
    "                    gmed_ft = hmean_ft = np.nan + 1j*np.nan\n",
    "                else:\n",
    "                    gmed_ft = geometric_median(xd_data_bft, init_guess=gmed_ft_init, \\\n",
    "                                               options=dict(keep_res=True))\n",
    "                    gmed_ft_init = gmed_ft\n",
    "                    hmean_ft = rsc_mean(xd_data_bft, sigma=mad_sigma)\n",
    "\n",
    "                xd_gmed_res_bl_f[:, tint, b] = gmed_ft\n",
    "                xd_hmean_res_bl_f[:, tint, b] = hmean_ft\n",
    "\n",
    "        return xd_gmed_res_bl_f, xd_hmean_res_bl_f\n",
    "\n",
    "    if mp:\n",
    "        m_pool = multiprocessing.Pool(multiprocessing.cpu_count())\n",
    "        pool_res = m_pool.map(freq_iter, range(no_chans))\n",
    "        m_pool.close()\n",
    "        m_pool.join()\n",
    "    else:\n",
    "        pool_res = list(map(freq_iter, range(no_chans)))\n",
    "\n",
    "    loc_res = np.concatenate(pool_res, axis=1)\n",
    "    xd_gmed_res_bl =  np.concatenate((xd_gmed_res_t, loc_res[0, ...]), axis=-1)\n",
    "    xd_hmean_res_bl = np.concatenate((xd_hmean_res_t, loc_res[1, ...]), axis=-1)\n",
    "\n",
    "    np.savez(lst_full_res_fn, xd_gmed_res_bl=xd_gmed_res_bl, xd_hmean_res_bl=xd_hmean_res_bl)\n",
    "\n",
    "else:\n",
    "    red_res = np.load(lst_full_res_fn)\n",
    "    xd_gmed_res_bl = red_res['xd_gmed_res_bl']\n",
    "    xd_hmean_res_bl = red_res['xd_hmean_res_bl']\n",
    "\n",
    "# remove baselines with only nan entries\n",
    "nan_bls = np.where(np.isnan(xd_data_bls).all(axis=(0, 1, 2)))[0]\n",
    "flt_no_bls = no_bls - nan_bls.size\n",
    "xd_gmed_res_bl = np.delete(xd_gmed_res_bl, nan_bls, axis=2)\n",
    "xd_hmean_res_bl = np.delete(xd_hmean_res_bl, nan_bls, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the visibility location estimates for a selected time slice\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 8), sharey='row', sharex='col')\n",
    "\n",
    "slct_tint = 0\n",
    "\n",
    "axes[0][0].plot(np.abs(xd_gmed_res_bl[:, slct_tint, :]), alpha=0.7)\n",
    "axes[0][1].plot(np.abs(xd_hmean_res_bl[:, slct_tint, :]), alpha=0.7)\n",
    "axes[1][0].plot(np.angle(xd_gmed_res_bl[:, slct_tint, :]), alpha=0.7)\n",
    "axes[1][1].plot(np.angle(xd_hmean_res_bl[:, slct_tint, :]), alpha=0.7)\n",
    "\n",
    "axes[0][0].set_ylabel(r'$|V|$')\n",
    "axes[1][0].set_ylabel(r'$\\varphi$')\n",
    "\n",
    "axes[1][0].set_xlabel('Frequency Channel')\n",
    "axes[1][1].set_xlabel('Frequency Channel')\n",
    "\n",
    "axes[0][0].set_title('Geometric Median')\n",
    "axes[0][1].set_title('HERA Mean')\n",
    "\n",
    "for axr in axes:\n",
    "    for axc in axr:\n",
    "        axc.set_xticks(np.arange(plt_chans.size)[::fbase])\n",
    "        axc.set_xticklabels(plt_chans[::fbase])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42851ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D plot of visibility amplitudes across all redundant baselines in time and frequency\n",
    "# look at geometric median estimates\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = plt.axes(projection='3d')\n",
    "for tint in range(new_no_tints):\n",
    "    for bl in range(xd_gmed_res_bl.shape[2]):\n",
    "        ax.plot(chans, np.repeat(tint, chans.size), np.abs(xd_gmed_res_bl[:, tint, bl]))\n",
    "ax.set_xlabel('Frequency Channel')\n",
    "ax.set_ylabel('Time bin')\n",
    "ax.set_zlabel(r'$|V|$')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcedab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D interpolation for each baseline separately\n",
    "gmed_interp_bl_list = []\n",
    "hmean_interp_bl_list = []\n",
    "nan_idxs_f = []\n",
    "nan_idxs_t = []\n",
    "\n",
    "for bl in range(flt_no_bls):\n",
    "    gmed_i, gmed_nidxf, gmed_nidxt = nan_interp2d(xd_gmed_res_bl[..., bl], rtn_nan_idxs=True)\n",
    "    hmean_i, hmean_nidxf, hmean_nidxt = nan_interp2d(xd_hmean_res_bl[..., bl], rtn_nan_idxs=True)\n",
    "    gmed_interp_bl_list.append(gmed_i)\n",
    "    hmean_interp_bl_list.append(hmean_i)\n",
    "    nan_idxs_f.append(gmed_nidxf)\n",
    "    nan_idxs_f.append(hmean_nidxf)\n",
    "    nan_idxs_t.append(gmed_nidxt)\n",
    "    nan_idxs_t.append(hmean_nidxt)\n",
    "\n",
    "if np.unique(nan_idxs_f).size != 0:\n",
    "    gmed_interp_bl_list = [np.delete(gmed_i, np.unique(nan_idxs_f), axis=0) for gmed_i in gmed_interp_bl_list]\n",
    "    hmean_interp_bl_list = [np.delete(hmean_i, np.unique(nan_idxs_f), axis=0) for hmean_i in hmean_interp_bl_list]\n",
    "\n",
    "if np.unique(nan_idxs_t).size != 0:\n",
    "    gmed_interp_bl_list = [np.delete(gmed_i, np.unique(nan_idxs_t), axis=1) for gmed_i in gmed_interp_bl_list]\n",
    "    hmean_interp_bl_list = [np.delete(hmean_i, np.unique(nan_idxs_t), axis=1) for hmean_i in hmean_interp_bl_list]\n",
    "\n",
    "gmed_interp2_bl = np.moveaxis(np.array(gmed_interp_bl_list), 0, 2)\n",
    "hmean_interp2_bl = np.moveaxis(np.array(hmean_interp_bl_list), 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-PS between all baseline pairs\n",
    "bl_pairs = list(itertools.permutations(np.arange(flt_no_bls), r=2))\n",
    "bls1 = [i[0] for i in bl_pairs]\n",
    "bls2 = [i[1] for i in bl_pairs]\n",
    "\n",
    "gmed_delay, gmed_pspec = signal.csd(gmed_interp2_bl[..., bls1], gmed_interp2_bl[..., bls2], \\\n",
    "    fs=1/f_resolution, window='hann', scaling='spectrum', nfft=None, detrend=False, \\\n",
    "    nperseg=gmed_interp2_bl.shape[0], return_onesided=False, axis=0)\n",
    "\n",
    "delay_sort = np.argsort(gmed_delay)\n",
    "gmed_delay = gmed_delay[delay_sort]\n",
    "gmed_pspec = gmed_pspec[delay_sort, :]\n",
    "\n",
    "hmean_delay, hmean_pspec = signal.csd(hmean_interp2_bl[..., bls1], hmean_interp2_bl[..., bls2], \\\n",
    "    fs=1/f_resolution, window='hann', scaling='spectrum', nfft=None, detrend=False, \\\n",
    "    nperseg=hmean_interp2_bl.shape[0], return_onesided=False, axis=0)\n",
    "\n",
    "delay_sort = np.argsort(hmean_delay)\n",
    "hmean_delay = hmean_delay[delay_sort]\n",
    "hmean_pspec = hmean_pspec[delay_sort, :]\n",
    "\n",
    "gmed_pspec = np.nanmean(gmed_pspec, axis=2)\n",
    "hmean_pspec = np.nanmean(hmean_pspec, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f31eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(10, 6), sharey=True)\n",
    "\n",
    "axes[0].plot(gmed_delay*1e6, np.abs(gmed_pspec), alpha=0.3)\n",
    "axes[0].plot(gmed_delay*1e6, np.abs(gmed_pspec.mean(axis=1)), alpha=1, color='orange')\n",
    "axes[0].set_ylabel(r'Power Spectrum [Jy$^2$ Hz$^2$]')\n",
    "\n",
    "axes[1].plot(hmean_delay*1e6, np.abs(hmean_pspec), alpha=0.3)\n",
    "axes[1].plot(hmean_delay*1e6, np.abs(hmean_pspec.mean(axis=1)), alpha=1, color='purple')\n",
    "\n",
    "# average over times\n",
    "axes[2].plot(gmed_delay*1e6, np.abs(gmed_pspec.mean(axis=1)), alpha=0.6, color='orange', label='Geometric Median')\n",
    "axes[2].plot(hmean_delay*1e6, np.abs(hmean_pspec.mean(axis=1)), alpha=0.6, color='purple', label='HERA Mean')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel(r'Delay [$\\mu$s]')\n",
    "\n",
    "axes[0].set_title('Geometric Median')\n",
    "axes[1].set_title('HERA Mean')\n",
    "axes[2].set_title('Comparison')\n",
    "axes[2].legend(loc='lower center')\n",
    "# plt.suptitle('Power spectra')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
